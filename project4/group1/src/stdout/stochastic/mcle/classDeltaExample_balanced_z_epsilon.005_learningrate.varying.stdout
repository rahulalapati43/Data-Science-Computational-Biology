train rr files ===================
../../data/1jfu.rr ../../data/1wkc.rr ../../data/1nrv.rr ../../data/1tqg.rr ../../data/1h98.rr ../../data/1ne2.rr ../../data/1pko.rr ../../data/2tps.rr ../../data/1jbe.rr ../../data/1cxy.rr ../../data/3bor.rr ../../data/1k6k.rr ../../data/1whi.rr ../../data/1j3a.rr ../../data/1jo0.rr ../../data/1iib.rr ../../data/1czn.rr ../../data/1dix.rr ../../data/1ny1.rr ../../data/1k7c.rr ../../data/1jvw.rr ../../data/1vfy.rr ../../data/1avs.rr ../../data/1dlw.rr ../../data/1t8k.rr ../../data/1d1q.rr ../../data/1h0p.rr ../../data/1fx2.rr ../../data/1cc8.rr ../../data/1ej8.rr ../../data/1vhu.rr ../../data/1d4o.rr ../../data/2hs1.rr ../../data/1lm4.rr ../../data/1jl1.rr ../../data/1fl0.rr ../../data/1hdo.rr ../../data/1hxn.rr ../../data/1jyh.rr ../../data/1jo8.rr ../../data/1m4j.rr ../../data/1d0q.rr ../../data/1c9o.rr ../../data/2vxn.rr ../../data/1vp6.rr ../../data/1xff.rr ../../data/1h2e.rr ../../data/1nb9.rr ../../data/1beb.rr ../../data/1chd.rr ../../data/1gmx.rr ../../data/1jfx.rr ../../data/1c44.rr ../../data/1kq6.rr ../../data/2phy.rr ../../data/1fcy.rr ../../data/1tzv.rr ../../data/1kqr.rr ../../data/1p90.rr ../../data/1mug.rr ../../data/1tqh.rr ../../data/1a6m.rr ../../data/1k7j.rr ../../data/1atz.rr ../../data/1nps.rr ../../data/1qjp.rr ../../data/1o1z.rr ../../data/1bsg.rr ../../data/1beh.rr ../../data/1cke.rr ../../data/1rw7.rr ../../data/1ryb.rr ../../data/1i71.rr ../../data/1lpy.rr ../../data/1roa.rr ../../data/1aap.rr ../../data/1xkr.rr ../../data/1rw1.rr ../../data/1kid.rr ../../data/1ktg.rr ../../data/1vmb.rr ../../data/1hh8.rr ../../data/1htw.rr ../../data/1gz2.rr ../../data/1jkx.rr ../../data/1ihz.rr ../../data/1wjx.rr ../../data/1bkr.rr ../../data/1ej0.rr ../../data/1fvg.rr ../../data/1vjk.rr ../../data/1m8a.rr ../../data/1jwq.rr ../../data/3dqg.rr ../../data/1f6b.rr ../../data/1im5.rr ../../data/1gbs.rr ../../data/1dsx.rr ../../data/1ctf.rr ../../data/1svy.rr ../../data/1ql0.rr ../../data/1iwd.rr ../../data/1dmg.rr ../../data/1guu.rr ../../data/1eaz.rr ../../data/1fvk.rr ../../data/1smx.rr ../../data/1fk5.rr ../../data/1gmi.rr ../../data/1r26.rr ../../data/1qf9.rr ../../data/1fna.rr
test rr files ====================
../../data/1gzc.rr ../../data/1jos.rr ../../data/5ptp.rr ../../data/1dbx.rr ../../data/1lo7.rr ../../data/1w0h.rr ../../data/1aba.rr ../../data/1kw4.rr ../../data/1h4x.rr ../../data/1i1j.rr ../../data/1hfc.rr ../../data/1tif.rr ../../data/1g2r.rr ../../data/2mhr.rr ../../data/1pch.rr ../../data/1bdo.rr ../../data/2cua.rr ../../data/1c52.rr ../../data/1aoe.rr ../../data/1ag6.rr ../../data/1ku3.rr ../../data/1cjw.rr ../../data/1a3a.rr ../../data/1mk0.rr ../../data/1ek0.rr ../../data/1i4j.rr ../../data/1i5g.rr ../../data/1xdz.rr ../../data/1dqg.rr ../../data/2arc.rr ../../data/1fqt.rr ../../data/1jbk.rr ../../data/1a70.rr ../../data/1i58.rr ../../data/1g9o.rr ../../data/1i1n.rr ../../data/1brf.rr ../../data/1atl.rr
Running stochastic gradient descent
iteration #0: max delta: 0.0225 ==== learning rate was 0.045
iteration #1: max delta: 0.0228066666932 ==== learning rate was 0.0449458845156
iteration #2: max delta: 0.0231696854776 ==== learning rate was 0.0448918422434
iteration #3: max delta: 0.0234784723018 ==== learning rate was 0.0448378730843
iteration #4: max delta: 0.0237624830216 ==== learning rate was 0.0447839769393
iteration #5: max delta: 0.0241870718683 ==== learning rate was 0.0447301537098
iteration #6: max delta: 0.0244020795221 ==== learning rate was 0.044676403297
iteration #7: max delta: 0.0197376985127 ==== learning rate was 0.0446227256025
iteration #8: max delta: 0.0201111608799 ==== learning rate was 0.0445691205278
iteration #9: max delta: 0.0205214179117 ==== learning rate was 0.0445155879747
iteration #10: max delta: 0.0206196255654 ==== learning rate was 0.0444621278451
iteration #11: max delta: 0.0235613067484 ==== learning rate was 0.044408740041
iteration #12: max delta: 0.0206094095871 ==== learning rate was 0.0443554244647
iteration #13: max delta: 0.0235240069524 ==== learning rate was 0.0443021810182
iteration #14: max delta: 0.0238714115934 ==== learning rate was 0.0442490096042
iteration #15: max delta: 0.0241194056387 ==== learning rate was 0.044195910125
iteration #16: max delta: 0.0195004636045 ==== learning rate was 0.0441428824835
iteration #17: max delta: 0.0200944952835 ==== learning rate was 0.0440899265824
iteration #18: max delta: 0.0239864643223 ==== learning rate was 0.0440370423246
iteration #19: max delta: 0.0198157336947 ==== learning rate was 0.0439842296133
iteration #20: max delta: 0.020086375811 ==== learning rate was 0.0439314883516
iteration #21: max delta: 0.0233795313616 ==== learning rate was 0.0438788184429
iteration #22: max delta: 0.0202347339066 ==== learning rate was 0.0438262197906
iteration #23: max delta: 0.0236756230501 ==== learning rate was 0.0437736922983
iteration #24: max delta: 0.0200322437878 ==== learning rate was 0.0437212358698
iteration #25: max delta: 0.0202699624733 ==== learning rate was 0.043668850409
iteration #26: max delta: 0.0234351193364 ==== learning rate was 0.0436165358198
iteration #27: max delta: 0.0238560020547 ==== learning rate was 0.0435642920062
iteration #28: max delta: 0.0240174058338 ==== learning rate was 0.0435121188727
iteration #29: max delta: 0.0244869449808 ==== learning rate was 0.0434600163236
iteration #30: max delta: 0.0245859178667 ==== learning rate was 0.0434079842633
iteration #31: max delta: 0.0247967324257 ==== learning rate was 0.0433560225965
iteration #32: max delta: 0.0253180381772 ==== learning rate was 0.0433041312279
iteration #33: max delta: 0.0249133765099 ==== learning rate was 0.0432523100626
iteration #34: max delta: 0.0259493180985 ==== learning rate was 0.0432005590054
iteration #35: max delta: 0.0267276441053 ==== learning rate was 0.0431488779616
iteration #36: max delta: 0.026655224886 ==== learning rate was 0.0430972668364
iteration #37: max delta: 0.0164433083881 ==== learning rate was 0.0430457255352
iteration #38: max delta: 0.0166690696369 ==== learning rate was 0.0429942539635
iteration #39: max delta: 0.0168677802218 ==== learning rate was 0.0429428520271
iteration #40: max delta: 0.0168452188076 ==== learning rate was 0.0428915196317
iteration #41: max delta: 0.017289454828 ==== learning rate was 0.0428402566833
iteration #42: max delta: 0.0174261832319 ==== learning rate was 0.0427890630878
iteration #43: max delta: 0.0253153714176 ==== learning rate was 0.0427379387515
iteration #44: max delta: 0.0172722696229 ==== learning rate was 0.0426868835806
iteration #45: max delta: 0.0249797850685 ==== learning rate was 0.0426358974817
iteration #46: max delta: 0.0169314423676 ==== learning rate was 0.0425849803611
iteration #47: max delta: 0.0255136734501 ==== learning rate was 0.0425341321257
iteration #48: max delta: 0.0166225663968 ==== learning rate was 0.0424833526822
iteration #49: max delta: 0.017342757791 ==== learning rate was 0.0424326419376
iteration #50: max delta: 0.0246220673651 ==== learning rate was 0.0423819997988
iteration #51: max delta: 0.025436761381 ==== learning rate was 0.0423314261732
iteration #52: max delta: 0.0171171123544 ==== learning rate was 0.042280920968
iteration #53: max delta: 0.0166584170939 ==== learning rate was 0.0422304840906
iteration #54: max delta: 0.0172680740357 ==== learning rate was 0.0421801154486
iteration #55: max delta: 0.0252314297065 ==== learning rate was 0.0421298149497
iteration #56: max delta: 0.0249174414236 ==== learning rate was 0.0420795825017
iteration #57: max delta: 0.0258026397912 ==== learning rate was 0.0420294180125
iteration #58: max delta: 0.0161398819331 ==== learning rate was 0.0419793213902
iteration #59: max delta: 0.0260549710042 ==== learning rate was 0.041929292543
iteration #60: max delta: 0.0161195595043 ==== learning rate was 0.0418793313792
iteration #61: max delta: 0.0252796981173 ==== learning rate was 0.0418294378072
iteration #62: max delta: 0.0258085919335 ==== learning rate was 0.0417796117355
iteration #63: max delta: 0.0263379792319 ==== learning rate was 0.0417298530729
iteration #64: max delta: 0.0157325038842 ==== learning rate was 0.0416801617282
iteration #65: max delta: 0.0259212541526 ==== learning rate was 0.0416305376102
iteration #66: max delta: 0.0262490551021 ==== learning rate was 0.0415809806281
iteration #67: max delta: 0.0141039137038 ==== learning rate was 0.041531490691
iteration #68: max delta: 0.014687219122 ==== learning rate was 0.0414820677081
iteration #69: max delta: 0.0268873893526 ==== learning rate was 0.041432711589
iteration #70: max delta: 0.0143939254147 ==== learning rate was 0.0413834222432
iteration #71: max delta: 0.0146183105387 ==== learning rate was 0.0413341995802
iteration #72: max delta: 0.0262862995097 ==== learning rate was 0.04128504351
iteration #73: max delta: 0.0141450628945 ==== learning rate was 0.0412359539424
iteration #74: max delta: 0.0146355993896 ==== learning rate was 0.0411869307874
iteration #75: max delta: 0.0261907526764 ==== learning rate was 0.0411379739552
iteration #76: max delta: 0.0149700056197 ==== learning rate was 0.0410890833561
iteration #77: max delta: 0.0261997534375 ==== learning rate was 0.0410402589005
iteration #78: max delta: 0.014169405187 ==== learning rate was 0.0409915004988
iteration #79: max delta: 0.0147199513752 ==== learning rate was 0.0409428080618
iteration #80: max delta: 0.0267466301067 ==== learning rate was 0.0408941815001
iteration #81: max delta: 0.0265010778473 ==== learning rate was 0.0408456207246
iteration #82: max delta: 0.0272724816198 ==== learning rate was 0.0407971256464
iteration #83: max delta: 0.0135641918178 ==== learning rate was 0.0407486961766
iteration #84: max delta: 0.0267053487555 ==== learning rate was 0.0407003322264
iteration #85: max delta: 0.0132285673899 ==== learning rate was 0.0406520337071
iteration #86: max delta: 0.0272876886389 ==== learning rate was 0.0406038005303
iteration #87: max delta: 0.0133994321809 ==== learning rate was 0.0405556326075
iteration #88: max delta: 0.0275416301319 ==== learning rate was 0.0405075298505
iteration #89: max delta: 0.0276949445589 ==== learning rate was 0.0404594921711
iteration #90: max delta: 0.0133705244217 ==== learning rate was 0.0404115194813
iteration #91: max delta: 0.0129679817083 ==== learning rate was 0.0403636116931
iteration #92: max delta: 0.028092693428 ==== learning rate was 0.0403157687187
iteration #93: max delta: 0.0275261148226 ==== learning rate was 0.0402679904704
iteration #94: max delta: 0.0123579474539 ==== learning rate was 0.0402202768608
iteration #95: max delta: 0.0121485101293 ==== learning rate was 0.0401726278022
iteration #96: max delta: 0.0127142771949 ==== learning rate was 0.0401250432075
iteration #97: max delta: 0.0277261790996 ==== learning rate was 0.0400775229893
iteration #98: max delta: 0.0282244500035 ==== learning rate was 0.0400300670606
iteration #99: max delta: 0.0119264234578 ==== learning rate was 0.0399826753345
iteration #100: max delta: 0.028245450174 ==== learning rate was 0.039935347724
iteration #101: max delta: 0.0120829221588 ==== learning rate was 0.0398880841424
iteration #102: max delta: 0.0115624564607 ==== learning rate was 0.0398408845031
iteration #103: max delta: 0.0124323003592 ==== learning rate was 0.0397937487196
iteration #104: max delta: 0.0276292771991 ==== learning rate was 0.0397466767055
iteration #105: max delta: 0.0277065568289 ==== learning rate was 0.0396996683745
iteration #106: max delta: 0.0118989165682 ==== learning rate was 0.0396527236404
iteration #107: max delta: 0.0121007284163 ==== learning rate was 0.0396058424173
iteration #108: max delta: 0.0282629524988 ==== learning rate was 0.0395590246192
iteration #109: max delta: 0.0286322044387 ==== learning rate was 0.0395122701603
iteration #110: max delta: 0.0113709550217 ==== learning rate was 0.0394655789549
iteration #111: max delta: 0.0290327185005 ==== learning rate was 0.0394189509175
iteration #112: max delta: 0.0110567064828 ==== learning rate was 0.0393723859624
iteration #113: max delta: 0.0284261160667 ==== learning rate was 0.0393258840046
iteration #114: max delta: 0.0282030262257 ==== learning rate was 0.0392794449586
iteration #115: max delta: 0.0290954593149 ==== learning rate was 0.0392330687394
iteration #116: max delta: 0.00980495971302 ==== learning rate was 0.039186755262
iteration #117: max delta: 0.0289653542738 ==== learning rate was 0.0391405044415
iteration #118: max delta: 0.00931332657165 ==== learning rate was 0.0390943161932
iteration #119: max delta: 0.009611474029 ==== learning rate was 0.0390481904323
iteration #120: max delta: 0.0296986116042 ==== learning rate was 0.0390021270744
iteration #121: max delta: 0.0291139256349 ==== learning rate was 0.038956126035
iteration #122: max delta: 0.00957009273261 ==== learning rate was 0.0389101872298
iteration #123: max delta: 0.00924267895216 ==== learning rate was 0.0388643105746
iteration #124: max delta: 0.0294730985342 ==== learning rate was 0.0388184959854
iteration #125: max delta: 0.0300398263234 ==== learning rate was 0.0387727433781
iteration #126: max delta: 0.00925201272699 ==== learning rate was 0.038727052669
iteration #127: max delta: 0.00883570750448 ==== learning rate was 0.0386814237742
iteration #128: max delta: 0.00895356959662 ==== learning rate was 0.0386358566101
iteration #129: max delta: 0.00972588922247 ==== learning rate was 0.0385903510933
iteration #130: max delta: 0.0293025638681 ==== learning rate was 0.0385449071404
iteration #131: max delta: 0.0290659893631 ==== learning rate was 0.0384995246679
iteration #132: max delta: 0.0301944628231 ==== learning rate was 0.0384542035928
iteration #133: max delta: 0.00802123936337 ==== learning rate was 0.038408943832
iteration #134: max delta: 0.0298982545015 ==== learning rate was 0.0383637453026
iteration #135: max delta: 0.00773847050649 ==== learning rate was 0.0383186079217
iteration #136: max delta: 0.00887964956857 ==== learning rate was 0.0382735316065
iteration #137: max delta: 0.00823061514863 ==== learning rate was 0.0382285162746
iteration #138: max delta: 0.00870430052608 ==== learning rate was 0.0381835618432
iteration #139: max delta: 0.00825333259459 ==== learning rate was 0.0381386682302
iteration #140: max delta: 0.00902726721373 ==== learning rate was 0.0380938353531
iteration #141: max delta: 0.0303065661197 ==== learning rate was 0.0380490631299
iteration #142: max delta: 0.0301454048286 ==== learning rate was 0.0380043514784
iteration #143: max delta: 0.00805005667753 ==== learning rate was 0.0379597003168
iteration #144: max delta: 0.00786016771803 ==== learning rate was 0.0379151095631
iteration #145: max delta: 0.00819577598409 ==== learning rate was 0.0378705791357
iteration #146: max delta: 0.00758651245808 ==== learning rate was 0.0378261089529
iteration #147: max delta: 0.00836637804757 ==== learning rate was 0.0377816989332
iteration #148: max delta: 0.00907074189857 ==== learning rate was 0.0377373489953
iteration #149: max delta: 0.00911609923174 ==== learning rate was 0.0376930590579
iteration #150: max delta: 0.0296149470025 ==== learning rate was 0.0376488290397
iteration #151: max delta: 0.0293349993337 ==== learning rate was 0.0376046588598
iteration #152: max delta: 0.00750741295685 ==== learning rate was 0.0375605484371
iteration #153: max delta: 0.00837972096215 ==== learning rate was 0.0375164976909
iteration #154: max delta: 0.00832892510388 ==== learning rate was 0.0374725065403
iteration #155: max delta: 0.00754119640597 ==== learning rate was 0.0374285749049
iteration #156: max delta: 0.0298522992389 ==== learning rate was 0.0373847027039
iteration #157: max delta: 0.00776734853155 ==== learning rate was 0.0373408898571
iteration #158: max delta: 0.00787317856097 ==== learning rate was 0.0372971362841
iteration #159: max delta: 0.00872731745257 ==== learning rate was 0.0372534419048
iteration #160: max delta: 0.0296935005976 ==== learning rate was 0.037209806639
iteration #161: max delta: 0.0295134197578 ==== learning rate was 0.0371662304068
iteration #162: max delta: 0.00792192060279 ==== learning rate was 0.0371227131283
iteration #163: max delta: 0.0301561807091 ==== learning rate was 0.0370792547238
iteration #164: max delta: 0.0291403196157 ==== learning rate was 0.0370358551137
iteration #165: max delta: 0.0299105299155 ==== learning rate was 0.0369925142182
iteration #166: max delta: 0.0291504142157 ==== learning rate was 0.0369492319582
iteration #167: max delta: 0.00644109859157 ==== learning rate was 0.0369060082541
iteration #168: max delta: 0.00664428835912 ==== learning rate was 0.0368628430268
iteration #169: max delta: 0.00641098742386 ==== learning rate was 0.0368197361973
iteration #170: max delta: 0.0307411100158 ==== learning rate was 0.0367766876863
iteration #171: max delta: 0.0309503636313 ==== learning rate was 0.0367336974152
iteration #172: max delta: 0.0306063916615 ==== learning rate was 0.036690765305
iteration #173: max delta: 0.0065872736615 ==== learning rate was 0.0366478912772
iteration #174: max delta: 0.00608706140356 ==== learning rate was 0.0366050752531
iteration #175: max delta: 0.0308409285171 ==== learning rate was 0.0365623171542
iteration #176: max delta: 0.00633964685281 ==== learning rate was 0.0365196169022
iteration #177: max delta: 0.00613028624452 ==== learning rate was 0.0364769744188
iteration #178: max delta: 0.0307677318247 ==== learning rate was 0.0364343896258
iteration #179: max delta: 0.006190727237 ==== learning rate was 0.0363918624453
iteration #180: max delta: 0.0310193512976 ==== learning rate was 0.0363493927993
iteration #181: max delta: 0.0309229788484 ==== learning rate was 0.0363069806099
iteration #182: max delta: 0.031276695091 ==== learning rate was 0.0362646257994
iteration #183: max delta: 0.0312789478985 ==== learning rate was 0.0362223282902
iteration #184: max delta: 0.0311853137231 ==== learning rate was 0.0361800880048
iteration #185: max delta: 0.0312593899728 ==== learning rate was 0.0361379048656
iteration #186: max delta: 0.031049198775 ==== learning rate was 0.0360957787955
iteration #187: max delta: 0.0322287235488 ==== learning rate was 0.0360537097173
iteration #188: max delta: 0.0317931108582 ==== learning rate was 0.0360116975537
iteration #189: max delta: 0.0318861240639 ==== learning rate was 0.0359697422279
iteration #190: max delta: 0.0320924195382 ==== learning rate was 0.0359278436628
iteration #191: max delta: 0.0322271740645 ==== learning rate was 0.0358860017818
iteration #192: max delta: 0.0321004324306 ==== learning rate was 0.0358442165082
iteration #193: max delta: 0.0322096095097 ==== learning rate was 0.0358024877653
iteration #194: max delta: 0.00330699876654 ==== learning rate was 0.0357608154766
iteration #195: max delta: 0.0326466362575 ==== learning rate was 0.0357191995659
iteration #196: max delta: 0.00344338842233 ==== learning rate was 0.0356776399568
iteration #197: max delta: 0.00379484591896 ==== learning rate was 0.0356361365732
iteration #198: max delta: 0.00395088448667 ==== learning rate was 0.0355946893389
iteration #199: max delta: 0.00293035822726 ==== learning rate was 0.0355532981781
iteration #200: max delta: 0.00347164590251 ==== learning rate was 0.0355119630149
iteration #201: max delta: 0.00367364093865 ==== learning rate was 0.0354706837735
iteration #202: max delta: 0.00314816756876 ==== learning rate was 0.0354294603782
iteration #203: max delta: 0.00286501210427 ==== learning rate was 0.0353882927535
iteration #204: max delta: 0.0322828268801 ==== learning rate was 0.035347180824
iteration #205: max delta: 0.0321569830194 ==== learning rate was 0.0353061245142
iteration #206: max delta: 0.0324005314672 ==== learning rate was 0.0352651237491
iteration #207: max delta: 0.00308410612185 ==== learning rate was 0.0352241784533
iteration #208: max delta: 0.00322969244935 ==== learning rate was 0.0351832885519
iteration #209: max delta: 0.0323100377015 ==== learning rate was 0.0351424539699
iteration #210: max delta: 0.0322595375607 ==== learning rate was 0.0351016746325
iteration #211: max delta: 0.0322032503727 ==== learning rate was 0.0350609504649
iteration #212: max delta: 0.0322183529688 ==== learning rate was 0.0350202813925
iteration #213: max delta: 0.00280525846145 ==== learning rate was 0.0349796673408
iteration #214: max delta: 0.0327268577719 ==== learning rate was 0.0349391082353
iteration #215: max delta: 0.00236896448452 ==== learning rate was 0.0348986040018
iteration #216: max delta: 0.032572199366 ==== learning rate was 0.0348581545658
iteration #217: max delta: 0.0326677604361 ==== learning rate was 0.0348177598534
iteration #218: max delta: 0.00180553786761 ==== learning rate was 0.0347774197905
iteration #219: max delta: 0.0328863472859 ==== learning rate was 0.0347371343031
iteration #220: max delta: 0.00205545135243 ==== learning rate was 0.0346969033175
iteration #221: max delta: 0.00219728648952 ==== learning rate was 0.0346567267598
iteration #222: max delta: 0.0325115408356 ==== learning rate was 0.0346166045564
iteration #223: max delta: 0.00189041663341 ==== learning rate was 0.0345765366339
iteration #224: max delta: 0.0325773380793 ==== learning rate was 0.0345365229188
iteration #225: max delta: 0.00173677523625 ==== learning rate was 0.0344965633377
iteration #226: max delta: 0.00196860988874 ==== learning rate was 0.0344566578174
iteration #227: max delta: 0.0324436182004 ==== learning rate was 0.0344168062848
iteration #228: max delta: 0.0325726885102 ==== learning rate was 0.0343770086667
iteration #229: max delta: 0.00194521740555 ==== learning rate was 0.0343372648904
iteration #230: max delta: 0.0325610983542 ==== learning rate was 0.0342975748829
iteration #231: max delta: 0.0321785701529 ==== learning rate was 0.0342579385715
iteration #232: max delta: 0.00178525630307 ==== learning rate was 0.0342183558836
iteration #233: max delta: 0.00163739033217 ==== learning rate was 0.0341788267466
iteration #234: max delta: 0.00186283561967 ==== learning rate was 0.034139351088
iteration #235: max delta: 0.0016690132467 ==== learning rate was 0.0340999288356
iteration #236: max delta: 0.00214021002766 ==== learning rate was 0.034060559917
iteration #237: max delta: 0.0017330466386 ==== learning rate was 0.0340212442601
iteration #238: max delta: 0.0323764709513 ==== learning rate was 0.0339819817928
iteration #239: max delta: 0.0326409419931 ==== learning rate was 0.0339427724433
iteration #240: max delta: 0.0320165391797 ==== learning rate was 0.0339036161395
iteration #241: max delta: 0.00194155143354 ==== learning rate was 0.0338645128099
iteration #242: max delta: 0.0324408216768 ==== learning rate was 0.0338254623826
iteration #243: max delta: 0.00146524260693 ==== learning rate was 0.0337864647861
iteration #244: max delta: 0.0324212211016 ==== learning rate was 0.033747519949
iteration #245: max delta: 0.0325294379079 ==== learning rate was 0.0337086277998
iteration #246: max delta: 0.0324542804881 ==== learning rate was 0.0336697882674
iteration #247: max delta: 0.032421309499 ==== learning rate was 0.0336310012804
iteration #248: max delta: 0.00125367868107 ==== learning rate was 0.0335922667678
iteration #249: max delta: 0.0326202697052 ==== learning rate was 0.0335535846587
iteration #250: max delta: 0.00136881434537 ==== learning rate was 0.0335149548821
iteration #251: max delta: 0.000939925766643 ==== learning rate was 0.0334763773672
iteration #252: max delta: 0.032207226784 ==== learning rate was 0.0334378520434
iteration #253: max delta: 0.00110961786868 ==== learning rate was 0.0333993788399
iteration #254: max delta: 0.00104407813983 ==== learning rate was 0.0333609576864
iteration #255: max delta: 0.0324130666097 ==== learning rate was 0.0333225885124
iteration #256: max delta: 0.00110258449554 ==== learning rate was 0.0332842712475
iteration #257: max delta: 0.0320817355318 ==== learning rate was 0.0332460058215
iteration #258: max delta: 0.0322332919986 ==== learning rate was 0.0332077921643
iteration #259: max delta: 0.0323594404142 ==== learning rate was 0.033169630206
iteration #260: max delta: 0.000958565703991 ==== learning rate was 0.0331315198764
iteration #261: max delta: 0.00110957988212 ==== learning rate was 0.0330934611058
iteration #262: max delta: 0.0322297872953 ==== learning rate was 0.0330554538244
iteration #263: max delta: 0.0323825913598 ==== learning rate was 0.0330174979626
iteration #264: max delta: 0.000861899976404 ==== learning rate was 0.0329795934508
iteration #265: max delta: 0.00114031876587 ==== learning rate was 0.0329417402195
iteration #266: max delta: 0.000925698969898 ==== learning rate was 0.0329039381993
iteration #267: max delta: 0.0318334875183 ==== learning rate was 0.032866187321
iteration #268: max delta: 0.032151065038 ==== learning rate was 0.0328284875154
iteration #269: max delta: 0.000677178337196 ==== learning rate was 0.0327908387134
iteration #270: max delta: 0.000704795916848 ==== learning rate was 0.032753240846
iteration #271: max delta: 0.0009715440111 ==== learning rate was 0.0327156938442
iteration #272: max delta: 0.000655051353965 ==== learning rate was 0.0326781976393
iteration #273: max delta: 0.000690197387423 ==== learning rate was 0.0326407521625
iteration #274: max delta: 0.0317233321417 ==== learning rate was 0.0326033573452
iteration #275: max delta: 0.00101868718921 ==== learning rate was 0.0325660131189
iteration #276: max delta: 0.0319353049262 ==== learning rate was 0.0325287194151
iteration #277: max delta: 0.0318529740773 ==== learning rate was 0.0324914761655
iteration #278: max delta: 0.000772581240552 ==== learning rate was 0.0324542833018
iteration #279: max delta: 0.031738296358 ==== learning rate was 0.0324171407558
iteration #280: max delta: 0.000556969884378 ==== learning rate was 0.0323800484595
iteration #281: max delta: 0.000680255229014 ==== learning rate was 0.0323430063449
iteration #282: max delta: 0.0317487391672 ==== learning rate was 0.0323060143441
iteration #283: max delta: 0.000554856396398 ==== learning rate was 0.0322690723893
iteration #284: max delta: 0.000656035634229 ==== learning rate was 0.0322321804128
iteration #285: max delta: 0.0316779080301 ==== learning rate was 0.0321953383469
iteration #286: max delta: 0.000608759506933 ==== learning rate was 0.0321585461243
iteration #287: max delta: 0.00054537390552 ==== learning rate was 0.0321218036773
iteration #288: max delta: 0.0314705402099 ==== learning rate was 0.0320851109387
iteration #289: max delta: 0.0315901261319 ==== learning rate was 0.0320484678413
iteration #290: max delta: 0.00057299501465 ==== learning rate was 0.0320118743178
iteration #291: max delta: 0.000653202510195 ==== learning rate was 0.0319753303013
iteration #292: max delta: 0.000558210413081 ==== learning rate was 0.0319388357247
iteration #293: max delta: 0.000623720463227 ==== learning rate was 0.0319023905211
iteration #294: max delta: 0.0313275514225 ==== learning rate was 0.0318659946237
iteration #295: max delta: 0.000581864863473 ==== learning rate was 0.0318296479659
iteration #296: max delta: 0.000551586122588 ==== learning rate was 0.0317933504811
iteration #297: max delta: 0.00053126425906 ==== learning rate was 0.0317571021026
iteration #298: max delta: 0.000550826516474 ==== learning rate was 0.0317209027641
iteration #299: max delta: 0.0310722755313 ==== learning rate was 0.0316847523992
iteration #300: max delta: 0.0310150832717 ==== learning rate was 0.0316486509417
iteration #301: max delta: 0.0311784225696 ==== learning rate was 0.0316125983253
iteration #302: max delta: 0.000573206153693 ==== learning rate was 0.0315765944841
iteration #303: max delta: 0.000466340069672 ==== learning rate was 0.031540639352
iteration #304: max delta: 0.0311117562959 ==== learning rate was 0.0315047328632
iteration #305: max delta: 0.0310990908114 ==== learning rate was 0.0314688749518
iteration #306: max delta: 0.000398753113307 ==== learning rate was 0.0314330655521
iteration #307: max delta: 0.0306640757584 ==== learning rate was 0.0313973045984
iteration #308: max delta: 0.030957523105 ==== learning rate was 0.0313615920253
iteration #309: max delta: 0.030978547774 ==== learning rate was 0.0313259277673
iteration #310: max delta: 0.000490787805319 ==== learning rate was 0.031290311759
iteration #311: max delta: 0.0308718432278 ==== learning rate was 0.0312547439351
iteration #312: max delta: 0.0308992616095 ==== learning rate was 0.0312192242305
iteration #313: max delta: 0.000362146873503 ==== learning rate was 0.03118375258
iteration #314: max delta: 0.03080935302 ==== learning rate was 0.0311483289187
iteration #315: max delta: 0.000347541188258 ==== learning rate was 0.0311129531816
iteration #316: max delta: 0.000341398799646 ==== learning rate was 0.0310776253038
iteration #317: max delta: 0.0306313717914 ==== learning rate was 0.0310423452207
iteration #318: max delta: 0.0306733165996 ==== learning rate was 0.0310071128676
iteration #319: max delta: 0.0306065120033 ==== learning rate was 0.0309719281799
iteration #320: max delta: 0.000277297612462 ==== learning rate was 0.030936791093
iteration #321: max delta: 0.000285900508224 ==== learning rate was 0.0309017015427
iteration #322: max delta: 0.0305129238156 ==== learning rate was 0.0308666594646
iteration #323: max delta: 0.000403900698592 ==== learning rate was 0.0308316647944
iteration #324: max delta: 0.0305455582595 ==== learning rate was 0.0307967174681
iteration #325: max delta: 0.000252958864362 ==== learning rate was 0.0307618174216
iteration #326: max delta: 0.000232991707746 ==== learning rate was 0.0307269645909
iteration #327: max delta: 0.000252770372206 ==== learning rate was 0.0306921589122
iteration #328: max delta: 0.0304368577428 ==== learning rate was 0.0306574003216
iteration #329: max delta: 0.000244543106107 ==== learning rate was 0.0306226887554
iteration #330: max delta: 0.000311534195617 ==== learning rate was 0.0305880241501
iteration #331: max delta: 0.000321221057824 ==== learning rate was 0.0305534064421
iteration #332: max delta: 0.000202419455499 ==== learning rate was 0.0305188355679
iteration #333: max delta: 0.0302268744982 ==== learning rate was 0.0304843114643
iteration #334: max delta: 0.00025479319127 ==== learning rate was 0.0304498340678
iteration #335: max delta: 0.000294049706436 ==== learning rate was 0.0304154033154
iteration #336: max delta: 0.0301970986428 ==== learning rate was 0.0303810191438
iteration #337: max delta: 0.000228088822561 ==== learning rate was 0.0303466814902
iteration #338: max delta: 0.0300754778938 ==== learning rate was 0.0303123902916
iteration #339: max delta: 0.0300790204661 ==== learning rate was 0.0302781454851
iteration #340: max delta: 0.000230254011938 ==== learning rate was 0.0302439470079
iteration #341: max delta: 0.0299963831645 ==== learning rate was 0.0302097947974
iteration #342: max delta: 0.0300264478434 ==== learning rate was 0.030175688791
iteration #343: max delta: 0.000199187897407 ==== learning rate was 0.0301416289262
iteration #344: max delta: 0.000167429516946 ==== learning rate was 0.0301076151405
iteration #345: max delta: 0.0299287817232 ==== learning rate was 0.0300736473716
iteration #346: max delta: 0.029890272765 ==== learning rate was 0.0300397255573
iteration #347: max delta: 0.0298651719393 ==== learning rate was 0.0300058496354
iteration #348: max delta: 0.0298094880989 ==== learning rate was 0.0299720195438
iteration #349: max delta: 0.0298113252227 ==== learning rate was 0.0299382352205
iteration #350: max delta: 0.029785134225 ==== learning rate was 0.0299044966035
iteration #351: max delta: 0.0296963043928 ==== learning rate was 0.0298708036311
iteration #352: max delta: 0.0297239398908 ==== learning rate was 0.0298371562415
iteration #353: max delta: 0.0296226046257 ==== learning rate was 0.029803554373
iteration #354: max delta: 0.0296568439786 ==== learning rate was 0.029769997964
iteration #355: max delta: 0.0296047696369 ==== learning rate was 0.0297364869531
iteration #356: max delta: 0.0295746034005 ==== learning rate was 0.0297030212787
iteration #357: max delta: 0.0295301939319 ==== learning rate was 0.0296696008797
iteration #358: max delta: 0.00011963137765 ==== learning rate was 0.0296362256947
iteration #359: max delta: 0.000161772903154 ==== learning rate was 0.0296028956625
iteration #360: max delta: 0.029415175134 ==== learning rate was 0.0295696107221
iteration #361: max delta: 0.029456145144 ==== learning rate was 0.0295363708125
iteration #362: max delta: 0.000104602725829 ==== learning rate was 0.0295031758727
iteration #363: max delta: 0.0293528374467 ==== learning rate was 0.029470025842
iteration #364: max delta: 0.000125913004052 ==== learning rate was 0.0294369206595
iteration #365: max delta: 0.000125184916917 ==== learning rate was 0.0294038602645
iteration #366: max delta: 0.0292730460334 ==== learning rate was 0.0293708445965
iteration #367: max delta: 0.000113732096006 ==== learning rate was 0.0293378735951
iteration #368: max delta: 8.94357437144e-05 ==== learning rate was 0.0293049471996
iteration #369: max delta: 0.0291935709002 ==== learning rate was 0.0292720653499
iteration #370: max delta: 0.000148435502835 ==== learning rate was 0.0292392279855
iteration #371: max delta: 0.0291325746349 ==== learning rate was 0.0292064350465
iteration #372: max delta: 0.000106678571214 ==== learning rate was 0.0291736864725
iteration #373: max delta: 0.000127990728954 ==== learning rate was 0.0291409822037
iteration #374: max delta: 0.000103004584454 ==== learning rate was 0.0291083221801
iteration #375: max delta: 0.0289674303798 ==== learning rate was 0.0290757063418
iteration #376: max delta: 0.000118879274305 ==== learning rate was 0.0290431346291
iteration #377: max delta: 0.0289529385433 ==== learning rate was 0.0290106069822
iteration #378: max delta: 5.38233155695e-05 ==== learning rate was 0.0289781233415
iteration #379: max delta: 0.0288820361595 ==== learning rate was 0.0289456836475
iteration #380: max delta: 7.24466898508e-05 ==== learning rate was 0.0289132878408
iteration #381: max delta: 0.0287942124345 ==== learning rate was 0.0288809358619
iteration #382: max delta: 0.000123139772866 ==== learning rate was 0.0288486276517
iteration #383: max delta: 8.96316080661e-05 ==== learning rate was 0.0288163631507
iteration #384: max delta: 0.0287265474134 ==== learning rate was 0.0287841423001
iteration #385: max delta: 7.83360618989e-05 ==== learning rate was 0.0287519650405
iteration #386: max delta: 0.0286445766268 ==== learning rate was 0.0287198313132
iteration #387: max delta: 0.0285958943507 ==== learning rate was 0.0286877410592
iteration #388: max delta: 9.98842115998e-05 ==== learning rate was 0.0286556942197
iteration #389: max delta: 7.89840065922e-05 ==== learning rate was 0.0286236907359
iteration #390: max delta: 6.30752719523e-05 ==== learning rate was 0.0285917305493
iteration #391: max delta: 5.38747951122e-05 ==== learning rate was 0.0285598136011
iteration #392: max delta: 5.50773299653e-05 ==== learning rate was 0.028527939833
iteration #393: max delta: 0.0284223155568 ==== learning rate was 0.0284961091865
iteration #394: max delta: 0.028402350309 ==== learning rate was 0.0284643216033
iteration #395: max delta: 7.18369668433e-05 ==== learning rate was 0.0284325770251
iteration #396: max delta: 0.0283492243543 ==== learning rate was 0.0284008753937
iteration #397: max delta: 0.0283051593901 ==== learning rate was 0.028369216651
iteration #398: max delta: 5.88871581616e-05 ==== learning rate was 0.028337600739
iteration #399: max delta: 0.0282640764643 ==== learning rate was 0.0283060275999
iteration #400: max delta: 5.53364543671e-05 ==== learning rate was 0.0282744971756
iteration #401: max delta: 0.0282014849092 ==== learning rate was 0.0282430094084
iteration #402: max delta: 0.028175948577 ==== learning rate was 0.0282115642405
iteration #403: max delta: 0.0281240359371 ==== learning rate was 0.0281801616145
iteration #404: max delta: 0.02810958169 ==== learning rate was 0.0281488014727
iteration #405: max delta: 0.028078846929 ==== learning rate was 0.0281174837576
iteration #406: max delta: 5.2863792056e-05 ==== learning rate was 0.0280862084118
iteration #407: max delta: 4.60131261528e-05 ==== learning rate was 0.028054975378
iteration #408: max delta: 0.0279512023285 ==== learning rate was 0.0280237845991
iteration #409: max delta: 0.0279455093794 ==== learning rate was 0.0279926360177
iteration #410: max delta: 4.28720354132e-05 ==== learning rate was 0.0279615295768
iteration #411: max delta: 4.01148134507e-05 ==== learning rate was 0.0279304652194
iteration #412: max delta: 0.0278563272971 ==== learning rate was 0.0278994428886
iteration #413: max delta: 0.0278069668233 ==== learning rate was 0.0278684625275
iteration #414: max delta: 4.45469853473e-05 ==== learning rate was 0.0278375240794
iteration #415: max delta: 0.0277769764154 ==== learning rate was 0.0278066274875
iteration #416: max delta: 3.36789237492e-05 ==== learning rate was 0.0277757726951
iteration #417: max delta: 5.27579757356e-05 ==== learning rate was 0.0277449596459
iteration #418: max delta: 3.97679559917e-05 ==== learning rate was 0.0277141882832
iteration #419: max delta: 3.99921817306e-05 ==== learning rate was 0.0276834585506
iteration #420: max delta: 4.42524324235e-05 ==== learning rate was 0.027652770392
iteration #421: max delta: 0.0275897699834 ==== learning rate was 0.0276221237509
iteration #422: max delta: 4.11674121816e-05 ==== learning rate was 0.0275915185713
iteration #423: max delta: 3.8642428661e-05 ==== learning rate was 0.0275609547971
iteration #424: max delta: 4.01958509346e-05 ==== learning rate was 0.0275304323722
iteration #425: max delta: 0.0274685045258 ==== learning rate was 0.0274999512406
iteration #426: max delta: 0.0274378432289 ==== learning rate was 0.0274695113467
iteration #427: max delta: 0.0273978736193 ==== learning rate was 0.0274391126344
iteration #428: max delta: 3.18655521289e-05 ==== learning rate was 0.0274087550482
iteration #429: max delta: 0.0273428171694 ==== learning rate was 0.0273784385324
iteration #430: max delta: 0.0272927202959 ==== learning rate was 0.0273481630313
iteration #431: max delta: 3.48694732978e-05 ==== learning rate was 0.0273179284897
iteration #432: max delta: 0.0272634019186 ==== learning rate was 0.0272877348519
iteration #433: max delta: 0.0272293603144 ==== learning rate was 0.0272575820627
iteration #434: max delta: 2.67533934691e-05 ==== learning rate was 0.0272274700669
iteration #435: max delta: 0.0271769318967 ==== learning rate was 0.0271973988092
iteration #436: max delta: 2.56957676728e-05 ==== learning rate was 0.0271673682345
iteration #437: max delta: 2.8136769302e-05 ==== learning rate was 0.0271373782877
iteration #438: max delta: 0.0270884940394 ==== learning rate was 0.027107428914
iteration #439: max delta: 1.77409102495e-05 ==== learning rate was 0.0270775200584
iteration #440: max delta: 0.0270111839633 ==== learning rate was 0.0270476516662
iteration #441: max delta: 2.88014769382e-05 ==== learning rate was 0.0270178236824
iteration #442: max delta: 3.58505601103e-05 ==== learning rate was 0.0269880360526
iteration #443: max delta: 0.0269325555596 ==== learning rate was 0.0269582887221
iteration #444: max delta: 2.18614767442e-05 ==== learning rate was 0.0269285816363
iteration #445: max delta: 0.0268720662828 ==== learning rate was 0.0268989147409
iteration #446: max delta: 0.0268524568689 ==== learning rate was 0.0268692879815
iteration #447: max delta: 2.2230481973e-05 ==== learning rate was 0.0268397013037
iteration #448: max delta: 0.0267912617488 ==== learning rate was 0.0268101546533
iteration #449: max delta: 0.0267601627825 ==== learning rate was 0.0267806479762
iteration #450: max delta: 0.0267302648287 ==== learning rate was 0.0267511812184
iteration #451: max delta: 2.64800684955e-05 ==== learning rate was 0.0267217543257
iteration #452: max delta: 0.0266719787121 ==== learning rate was 0.0266923672443
iteration #453: max delta: 2.17396051108e-05 ==== learning rate was 0.0266630199203
iteration #454: max delta: 0.0266036504848 ==== learning rate was 0.0266337122999
iteration #455: max delta: 0.0265903618172 ==== learning rate was 0.0266044443294
iteration #456: max delta: 1.94624135649e-05 ==== learning rate was 0.0265752159551
iteration #457: max delta: 1.35133793161e-05 ==== learning rate was 0.0265460271236
iteration #458: max delta: 2.83273771051e-05 ==== learning rate was 0.0265168777813
iteration #459: max delta: 1.88515757742e-05 ==== learning rate was 0.0264877678747
iteration #460: max delta: 2.11109039982e-05 ==== learning rate was 0.0264586973505
iteration #461: max delta: 0.0264157828681 ==== learning rate was 0.0264296661555
iteration #462: max delta: 0.0263888006382 ==== learning rate was 0.0264006742364
iteration #463: max delta: 0.0263530035459 ==== learning rate was 0.0263717215401
iteration #464: max delta: 1.32181523553e-05 ==== learning rate was 0.0263428080135
iteration #465: max delta: 0.0262986526699 ==== learning rate was 0.0263139336037
iteration #466: max delta: 0.0262733609925 ==== learning rate was 0.0262850982577
iteration #467: max delta: 1.57754376367e-05 ==== learning rate was 0.0262563019226
iteration #468: max delta: 2.90112921354e-05 ==== learning rate was 0.0262275445458
iteration #469: max delta: 1.61605046585e-05 ==== learning rate was 0.0261988260744
iteration #470: max delta: 2.301765235e-05 ==== learning rate was 0.0261701464559
iteration #471: max delta: 0.0261174617999 ==== learning rate was 0.0261415056377
iteration #472: max delta: 0.0260964700192 ==== learning rate was 0.0261129035672
iteration #473: max delta: 1.0868458106e-05 ==== learning rate was 0.0260843401921
iteration #474: max delta: 1.34506422308e-05 ==== learning rate was 0.0260558154601
iteration #475: max delta: 0.0260136256332 ==== learning rate was 0.0260273293188
iteration #476: max delta: 1.10894994539e-05 ==== learning rate was 0.025998881716
iteration #477: max delta: 0.02595151087 ==== learning rate was 0.0259704725996
iteration #478: max delta: 1.48100382172e-05 ==== learning rate was 0.0259421019176
iteration #479: max delta: 0.0259039997096 ==== learning rate was 0.0259137696179
iteration #480: max delta: 0.0258470574399 ==== learning rate was 0.0258854756485
iteration #481: max delta: 0.025848681078 ==== learning rate was 0.0258572199578
iteration #482: max delta: 0.0258170875248 ==== learning rate was 0.0258290024938
iteration #483: max delta: 0.025792885201 ==== learning rate was 0.0258008232048
iteration #484: max delta: 0.0257646027287 ==== learning rate was 0.0257726820392
iteration #485: max delta: 8.1239823545e-06 ==== learning rate was 0.0257445789455
iteration #486: max delta: 8.41521236878e-06 ==== learning rate was 0.025716513872
iteration #487: max delta: 0.0256760275659 ==== learning rate was 0.0256884867675
iteration #488: max delta: 0.0256510131026 ==== learning rate was 0.0256604975804
iteration #489: max delta: 0.0256244801951 ==== learning rate was 0.0256325462596
iteration #490: max delta: 1.84068928177e-05 ==== learning rate was 0.0256046327537
iteration #491: max delta: 0.0255707557088 ==== learning rate was 0.0255767570117
iteration #492: max delta: 0.025540883241 ==== learning rate was 0.0255489189824
iteration #493: max delta: 0.0255152313124 ==== learning rate was 0.0255211186148
iteration #494: max delta: 0.0254862247101 ==== learning rate was 0.0254933558579
iteration #495: max delta: 1.51275789817e-05 ==== learning rate was 0.025465630661
iteration #496: max delta: 0.0254295239146 ==== learning rate was 0.0254379429731
iteration #497: max delta: 0.0254045735951 ==== learning rate was 0.0254102927435
iteration #498: max delta: 0.0253753254856 ==== learning rate was 0.0253826799216
iteration #499: max delta: 0.0253490433099 ==== learning rate was 0.0253551044566
iteration #500: max delta: 6.32650512809e-06 ==== learning rate was 0.0253275662982
iteration #501: max delta: 0.0252947582127 ==== learning rate was 0.0253000653958
iteration #502: max delta: 0.0252675760671 ==== learning rate was 0.025272601699
iteration #503: max delta: 7.73993687039e-06 ==== learning rate was 0.0252451751575
iteration #504: max delta: 0.0252094287501 ==== learning rate was 0.025217785721
iteration #505: max delta: 5.46947076491e-06 ==== learning rate was 0.0251904333394
iteration #506: max delta: 0.0251586271193 ==== learning rate was 0.0251631179624
iteration #507: max delta: 4.25117086342e-06 ==== learning rate was 0.02513583954
iteration #508: max delta: 0.0251039041802 ==== learning rate was 0.0251085980223
iteration #509: max delta: 9.3520405008e-06 ==== learning rate was 0.0250813933592
iteration #510: max delta: 0.0250477368434 ==== learning rate was 0.025054225501
iteration #511: max delta: 4.3359670082e-06 ==== learning rate was 0.0250270943978
iteration #512: max delta: 0.0249955276513 ==== learning rate was 0.025
iteration #513: max delta: 5.24294238549e-06 ==== learning rate was 0.0249729422578
iteration #514: max delta: 5.35171055003e-06 ==== learning rate was 0.0249459211217
iteration #515: max delta: 4.62669470555e-06 ==== learning rate was 0.0249189365421
iteration #516: max delta: 0.0248858304226 ==== learning rate was 0.0248919884697
iteration #517: max delta: 5.05882537265e-06 ==== learning rate was 0.0248650768549
iteration #518: max delta: 8.19266412786e-06 ==== learning rate was 0.0248382016485
iteration #519: max delta: 0.0248081932589 ==== learning rate was 0.0248113628012
iteration #520: max delta: 3.69603911649e-06 ==== learning rate was 0.0247845602639
iteration #521: max delta: 5.86900352815e-06 ==== learning rate was 0.0247577939873
iteration #522: max delta: 5.04929320067e-06 ==== learning rate was 0.0247310639226
iteration #523: max delta: 4.15858023756e-06 ==== learning rate was 0.0247043700205
iteration #524: max delta: 4.91910381341e-06 ==== learning rate was 0.0246777122323
iteration #525: max delta: 4.92225507914e-06 ==== learning rate was 0.0246510905091
iteration #526: max delta: 0.0246205044089 ==== learning rate was 0.0246245048021
iteration #527: max delta: 2.97269724048e-06 ==== learning rate was 0.0245979550625
iteration #528: max delta: 0.024567665944 ==== learning rate was 0.0245714412418
iteration #529: max delta: 0.0245402981928 ==== learning rate was 0.0245449632912
iteration #530: max delta: 2.70546504933e-06 ==== learning rate was 0.0245185211623
iteration #531: max delta: 0.0244882916176 ==== learning rate was 0.0244921148066
iteration #532: max delta: 8.57896577697e-06 ==== learning rate was 0.0244657441758
iteration #533: max delta: 2.67944638024e-06 ==== learning rate was 0.0244394092214
iteration #534: max delta: 0.0244105443255 ==== learning rate was 0.0244131098953
iteration #535: max delta: 3.87425028419e-06 ==== learning rate was 0.0243868461492
iteration #536: max delta: 4.38660970917e-06 ==== learning rate was 0.0243606179349
iteration #537: max delta: 4.34924143771e-06 ==== learning rate was 0.0243344252045
iteration #538: max delta: 4.96043406068e-06 ==== learning rate was 0.0243082679099
iteration #539: max delta: 0.0242789997404 ==== learning rate was 0.0242821460031
iteration #540: max delta: 0.0242533245631 ==== learning rate was 0.0242560594364
iteration #541: max delta: 0.0242251049102 ==== learning rate was 0.0242300081618
iteration #542: max delta: 0.0242003993852 ==== learning rate was 0.0242039921316
iteration #543: max delta: 2.62545294423e-06 ==== learning rate was 0.0241780112982
iteration #544: max delta: 0.0241486237663 ==== learning rate was 0.024152065614
iteration #545: max delta: 0.024123222794 ==== learning rate was 0.0241261550313
iteration #546: max delta: 0.024096812013 ==== learning rate was 0.0241002795027
iteration #547: max delta: 2.09993540493e-06 ==== learning rate was 0.0240744389808
iteration #548: max delta: 0.02404456842 ==== learning rate was 0.0240486334182
iteration #549: max delta: 3.19805746052e-06 ==== learning rate was 0.0240228627676
iteration #550: max delta: 0.0239946414515 ==== learning rate was 0.0239971269818
iteration #551: max delta: 4.41603385102e-06 ==== learning rate was 0.0239714260136
iteration #552: max delta: 0.0239436752266 ==== learning rate was 0.0239457598159
iteration #553: max delta: 2.16608271564e-06 ==== learning rate was 0.0239201283416
iteration #554: max delta: 4.81844757032e-06 ==== learning rate was 0.0238945315439
iteration #555: max delta: 2.5865222233e-06 ==== learning rate was 0.0238689693758
iteration #556: max delta: 4.35186514768e-06 ==== learning rate was 0.0238434417903
iteration #557: max delta: 0.0238159394203 ==== learning rate was 0.0238179487408
iteration #558: max delta: 0.0237888767199 ==== learning rate was 0.0237924901806
iteration #559: max delta: 2.27326630524e-06 ==== learning rate was 0.0237670660629
iteration #560: max delta: 0.0237398835721 ==== learning rate was 0.0237416763411
iteration #561: max delta: 3.56610564865e-06 ==== learning rate was 0.0237163209688
iteration #562: max delta: 0.0236888875035 ==== learning rate was 0.0236909998994
iteration #563: max delta: 0.0236638230468 ==== learning rate was 0.0236657130866
iteration #564: max delta: 2.37892034738e-06 ==== learning rate was 0.023640460484
iteration #565: max delta: 0.0236112805089 ==== learning rate was 0.0236152420453
iteration #566: max delta: 5.13682336107e-06 ==== learning rate was 0.0235900577243
iteration #567: max delta: 3.15590330912e-06 ==== learning rate was 0.0235649074748
iteration #568: max delta: 1.68169401943e-06 ==== learning rate was 0.0235397912508
iteration #569: max delta: 0.0235120310427 ==== learning rate was 0.0235147090062
iteration #570: max delta: 0.0234881170415 ==== learning rate was 0.0234896606951
iteration #571: max delta: 2.69385916425e-06 ==== learning rate was 0.0234646462715
iteration #572: max delta: 0.0234355104616 ==== learning rate was 0.0234396656896
iteration #573: max delta: 1.83657103296e-06 ==== learning rate was 0.0234147189036
iteration #574: max delta: 2.15938509127e-06 ==== learning rate was 0.0233898058678
iteration #575: max delta: 2.03772011859e-06 ==== learning rate was 0.0233649265365
iteration #576: max delta: 2.82327008354e-06 ==== learning rate was 0.0233400808641
iteration #577: max delta: 3.90580428678e-06 ==== learning rate was 0.0233152688051
iteration #578: max delta: 0.0232887619739 ==== learning rate was 0.023290490314
iteration #579: max delta: 0.0232639998865 ==== learning rate was 0.0232657453455
iteration #580: max delta: 3.28328211905e-06 ==== learning rate was 0.0232410338541
iteration #581: max delta: 1.97283872346e-06 ==== learning rate was 0.0232163557945
iteration #582: max delta: 0.0231895163263 ==== learning rate was 0.0231917111216
iteration #583: max delta: 2.1894612651e-06 ==== learning rate was 0.0231670997901
iteration #584: max delta: 1.67180692617e-06 ==== learning rate was 0.023142521755
iteration #585: max delta: 1.41533275619e-06 ==== learning rate was 0.0231179769712
iteration #586: max delta: 0.0230924619098 ==== learning rate was 0.0230934653937
iteration #587: max delta: 1.80412889856e-06 ==== learning rate was 0.0230689869776
iteration #588: max delta: 1.80761081049e-06 ==== learning rate was 0.0230445416781
iteration #589: max delta: 1.3613800535e-06 ==== learning rate was 0.0230201294502
iteration #590: max delta: 2.23461297737e-06 ==== learning rate was 0.0229957502494
iteration #591: max delta: 0.0229698343652 ==== learning rate was 0.0229714040309
iteration #592: max delta: 0.0229442967636 ==== learning rate was 0.02294709075
iteration #593: max delta: 1.85555357325e-06 ==== learning rate was 0.0229228103623
iteration #594: max delta: 0.022897357663 ==== learning rate was 0.0228985628232
iteration #595: max delta: 0.0228733414056 ==== learning rate was 0.0228743480883
iteration #596: max delta: 1.26404901004e-06 ==== learning rate was 0.0228501661132
iteration #597: max delta: 1.07861683105e-06 ==== learning rate was 0.0228260168536
iteration #598: max delta: 0.0227997683085 ==== learning rate was 0.0228019002652
iteration #599: max delta: 0.0227765306428 ==== learning rate was 0.0227778163038
iteration #600: max delta: 0.022751416795 ==== learning rate was 0.0227537649253
iteration #601: max delta: 1.11948859734e-06 ==== learning rate was 0.0227297460856
iteration #602: max delta: 2.64518900722e-06 ==== learning rate was 0.0227057597406
iteration #603: max delta: 2.70425089003e-06 ==== learning rate was 0.0226818058465
iteration #604: max delta: 0.0226568337536 ==== learning rate was 0.0226578843593
iteration #605: max delta: 0.0226331788828 ==== learning rate was 0.0226339952352
iteration #606: max delta: 1.41057145437e-06 ==== learning rate was 0.0226101384304
iteration #607: max delta: 0.0225843531023 ==== learning rate was 0.0225863139011
iteration #608: max delta: 0.0225616628333 ==== learning rate was 0.0225625216037
iteration #609: max delta: 0.022537876875 ==== learning rate was 0.0225387614947
iteration #610: max delta: 0.0225110602136 ==== learning rate was 0.0225150335303
iteration #611: max delta: 1.6352096787e-06 ==== learning rate was 0.0224913376672
iteration #612: max delta: 1.30860121505e-06 ==== learning rate was 0.022467673862
iteration #613: max delta: 0.0224426878524 ==== learning rate was 0.0224440420712
iteration #614: max delta: 9.92418564695e-07 ==== learning rate was 0.0224204422516
iteration #615: max delta: 1.59488228807e-06 ==== learning rate was 0.0223968743598
iteration #616: max delta: 2.12153077871e-06 ==== learning rate was 0.0223733383527
iteration #617: max delta: 1.60372455356e-06 ==== learning rate was 0.0223498341872
iteration #618: max delta: 0.0223254200026 ==== learning rate was 0.0223263618202
iteration #619: max delta: 0.0223021910614 ==== learning rate was 0.0223029212087
iteration #620: max delta: 0.0222776214901 ==== learning rate was 0.0222795123096
iteration #621: max delta: 0.0222555221287 ==== learning rate was 0.0222561350802
iteration #622: max delta: 9.33632690081e-07 ==== learning rate was 0.0222327894775
iteration #623: max delta: 9.92890146705e-07 ==== learning rate was 0.0222094754587
iteration #624: max delta: 0.0221854792477 ==== learning rate was 0.0221861929812
iteration #625: max delta: 1.95306429846e-06 ==== learning rate was 0.0221629420023
iteration #626: max delta: 0.0221390135752 ==== learning rate was 0.0221397224793
iteration #627: max delta: 6.92841390983e-07 ==== learning rate was 0.0221165343697
iteration #628: max delta: 0.0220923766111 ==== learning rate was 0.022093377631
iteration #629: max delta: 0.0220695555899 ==== learning rate was 0.0220702522208
iteration #630: max delta: 6.09933832855e-07 ==== learning rate was 0.0220471580966
iteration #631: max delta: 0.0220229630728 ==== learning rate was 0.0220240952161
iteration #632: max delta: 1.46278435257e-06 ==== learning rate was 0.0220010635372
iteration #633: max delta: 0.0219768409735 ==== learning rate was 0.0219780630175
iteration #634: max delta: 0.0219545029587 ==== learning rate was 0.0219550936149
iteration #635: max delta: 0.0219317234017 ==== learning rate was 0.0219321552873
iteration #636: max delta: 2.55993315251e-06 ==== learning rate was 0.0219092479927
iteration #637: max delta: 6.27763526164e-07 ==== learning rate was 0.0218863716891
iteration #638: max delta: 0.0218627651936 ==== learning rate was 0.0218635263345
iteration #639: max delta: 0.0218394619577 ==== learning rate was 0.0218407118871
iteration #640: max delta: 0.0218173942049 ==== learning rate was 0.0218179283051
iteration #641: max delta: 0.0217944902128 ==== learning rate was 0.0217951755467
iteration #642: max delta: 0.0217718428543 ==== learning rate was 0.0217724535702
iteration #643: max delta: 0.0217488135303 ==== learning rate was 0.021749762334
iteration #644: max delta: 0.0217266337685 ==== learning rate was 0.0217271017964
iteration #645: max delta: 0.0217035869808 ==== learning rate was 0.021704471916
iteration #646: max delta: 5.23842742435e-07 ==== learning rate was 0.0216818726513
iteration #647: max delta: 1.47368891045e-06 ==== learning rate was 0.0216593039608
iteration #648: max delta: 5.22995155218e-07 ==== learning rate was 0.0216367658033
iteration #649: max delta: 8.82914531714e-07 ==== learning rate was 0.0216142581373
iteration #650: max delta: 0.0215907178014 ==== learning rate was 0.0215917809216
iteration #651: max delta: 7.05875998417e-07 ==== learning rate was 0.0215693341151
iteration #652: max delta: 4.98962182856e-07 ==== learning rate was 0.0215469176766
iteration #653: max delta: 0.0215240889339 ==== learning rate was 0.0215245315649
iteration #654: max delta: 0.0215016123166 ==== learning rate was 0.0215021757392
iteration #655: max delta: 9.11035586046e-07 ==== learning rate was 0.0214798501584
iteration #656: max delta: 4.16252376339e-07 ==== learning rate was 0.0214575547815
iteration #657: max delta: 4.24722665281e-07 ==== learning rate was 0.0214352895678
iteration #658: max delta: 3.75480993456e-07 ==== learning rate was 0.0214130544764
iteration #659: max delta: 0.0213901479957 ==== learning rate was 0.0213908494666
iteration #660: max delta: 0.0213682954933 ==== learning rate was 0.0213686744977
iteration #661: max delta: 5.24278491922e-07 ==== learning rate was 0.0213465295289
iteration #662: max delta: 1.07747653396e-06 ==== learning rate was 0.0213244145199
iteration #663: max delta: 0.0213020089071 ==== learning rate was 0.0213023294299
iteration #664: max delta: 0.0212798078569 ==== learning rate was 0.0212802742186
iteration #665: max delta: 0.0212579227078 ==== learning rate was 0.0212582488455
iteration #666: max delta: 4.58882132209e-07 ==== learning rate was 0.0212362532702
iteration #667: max delta: 4.20020548808e-07 ==== learning rate was 0.0212142874524
iteration #668: max delta: 3.6680233447e-07 ==== learning rate was 0.0211923513519
iteration #669: max delta: 0.0211699687502 ==== learning rate was 0.0211704449285
iteration #670: max delta: 0.0211481332044 ==== learning rate was 0.021148568142
iteration #671: max delta: 0.021126319572 ==== learning rate was 0.0211267209524
iteration #672: max delta: 1.07294606614e-06 ==== learning rate was 0.0211049033195
iteration #673: max delta: 0.0210828503854 ==== learning rate was 0.0210831152034
iteration #674: max delta: 5.75421489429e-07 ==== learning rate was 0.0210613565642
iteration #675: max delta: 3.6950744305e-07 ==== learning rate was 0.0210396273619
iteration #676: max delta: 3.54158746931e-07 ==== learning rate was 0.0210179275568
iteration #677: max delta: 0.0209959857631 ==== learning rate was 0.0209962571091
iteration #678: max delta: 6.95066982747e-07 ==== learning rate was 0.0209746159791
iteration #679: max delta: 0.0209526340357 ==== learning rate was 0.0209530041271
iteration #680: max delta: 0.0209311354176 ==== learning rate was 0.0209314215134
iteration #681: max delta: 0.0209096545082 ==== learning rate was 0.0209098680986
iteration #682: max delta: 0.0208881395233 ==== learning rate was 0.0208883438432
iteration #683: max delta: 3.42030965714e-07 ==== learning rate was 0.0208668487076
iteration #684: max delta: 6.86440783036e-07 ==== learning rate was 0.0208453826525
iteration #685: max delta: 0.0208236550624 ==== learning rate was 0.0208239456386
iteration #686: max delta: 0.0208021349178 ==== learning rate was 0.0208025376265
iteration #687: max delta: 0.0207809461892 ==== learning rate was 0.0207811585771
iteration #688: max delta: 2.27580846514e-07 ==== learning rate was 0.0207598084511
iteration #689: max delta: 0.0207382386797 ==== learning rate was 0.0207384872094
iteration #690: max delta: 3.4754307016e-07 ==== learning rate was 0.0207171948129
iteration #691: max delta: 2.60732671256e-07 ==== learning rate was 0.0206959312227
iteration #692: max delta: 0.0206743616756 ==== learning rate was 0.0206746963997
iteration #693: max delta: 0.0206530390948 ==== learning rate was 0.020653490305
iteration #694: max delta: 0.0206321589928 ==== learning rate was 0.0206323128997
iteration #695: max delta: 0.0206110002522 ==== learning rate was 0.0206111641451
iteration #696: max delta: 0.0205897604036 ==== learning rate was 0.0205900440024
iteration #697: max delta: 2.14619695788e-07 ==== learning rate was 0.0205689524328
iteration #698: max delta: 0.0205474479158 ==== learning rate was 0.0205478893978
iteration #699: max delta: 0.0205266632789 ==== learning rate was 0.0205268548586
iteration #700: max delta: 2.45753831531e-07 ==== learning rate was 0.0205058487769
iteration #701: max delta: 2.96809322811e-07 ==== learning rate was 0.0204848711139
iteration #702: max delta: 0.0204637901821 ==== learning rate was 0.0204639218314
iteration #703: max delta: 0.0204428565027 ==== learning rate was 0.0204430008909
iteration #704: max delta: 1.88102993123e-07 ==== learning rate was 0.0204221082541
iteration #705: max delta: 0.0204011081313 ==== learning rate was 0.0204012438826
iteration #706: max delta: 3.66763129904e-07 ==== learning rate was 0.0203804077383
iteration #707: max delta: 2.86370257339e-07 ==== learning rate was 0.020359599783
iteration #708: max delta: 0.0203386059223 ==== learning rate was 0.0203388199784
iteration #709: max delta: 4.76521015469e-07 ==== learning rate was 0.0203180682866
iteration #710: max delta: 3.57360679201e-07 ==== learning rate was 0.0202973446695
iteration #711: max delta: 3.63126083293e-07 ==== learning rate was 0.0202766490891
iteration #712: max delta: 0.0202556204429 ==== learning rate was 0.0202559815074
iteration #713: max delta: 2.90678844421e-07 ==== learning rate was 0.0202353418867
iteration #714: max delta: 0.0202144744254 ==== learning rate was 0.0202147301891
iteration #715: max delta: 2.58838687241e-07 ==== learning rate was 0.0201941463767
iteration #716: max delta: 0.0201734164678 ==== learning rate was 0.020173590412
iteration #717: max delta: 1.65871622312e-07 ==== learning rate was 0.0201530622571
iteration #718: max delta: 0.0201323135564 ==== learning rate was 0.0201325618745
iteration #719: max delta: 0.0201118500634 ==== learning rate was 0.0201120892266
iteration #720: max delta: 1.30317942244e-07 ==== learning rate was 0.0200916442759
iteration #721: max delta: 0.0200709309559 ==== learning rate was 0.0200712269849
iteration #722: max delta: 0.0200505540983 ==== learning rate was 0.0200508373162
iteration #723: max delta: 1.92089793376e-07 ==== learning rate was 0.0200304752324
iteration #724: max delta: 4.51298104221e-07 ==== learning rate was 0.0200101406963
iteration #725: max delta: 0.0199896302895 ==== learning rate was 0.0199898336704
iteration #726: max delta: 0.0199693667938 ==== learning rate was 0.0199695541177
iteration #727: max delta: 0.0199491906098 ==== learning rate was 0.0199493020009
iteration #728: max delta: 0.0199288122187 ==== learning rate was 0.0199290772829
iteration #729: max delta: 0.0199087349826 ==== learning rate was 0.0199088799267
iteration #730: max delta: 1.35213531348e-07 ==== learning rate was 0.0198887098952
iteration #731: max delta: 1.44212963349e-07 ==== learning rate was 0.0198685671516
iteration #732: max delta: 2.12692882643e-07 ==== learning rate was 0.0198484516587
iteration #733: max delta: 0.0198282196646 ==== learning rate was 0.0198283633799
iteration #734: max delta: 1.91838513563e-07 ==== learning rate was 0.0198083022782
iteration #735: max delta: 1.40222337101e-07 ==== learning rate was 0.019788268317
iteration #736: max delta: 1.77821625423e-07 ==== learning rate was 0.0197682614594
iteration #737: max delta: 4.20823285749e-07 ==== learning rate was 0.0197482816688
iteration #738: max delta: 0.0197282210805 ==== learning rate was 0.0197283289087
iteration #739: max delta: 0.0197082351316 ==== learning rate was 0.0197084031424
iteration #740: max delta: 0.0196883902174 ==== learning rate was 0.0196885043334
iteration #741: max delta: 2.21718457561e-07 ==== learning rate was 0.0196686324452
iteration #742: max delta: 0.0196487000174 ==== learning rate was 0.0196487874415
iteration #743: max delta: 0.0196288607574 ==== learning rate was 0.0196289692858
iteration #744: max delta: 0.0196090387594 ==== learning rate was 0.0196091779418
iteration #745: max delta: 1.80229016585e-07 ==== learning rate was 0.0195894133733
iteration #746: max delta: 1.14804809596e-07 ==== learning rate was 0.019569675544
iteration #747: max delta: 0.0195498376921 ==== learning rate was 0.0195499644178
iteration #748: max delta: 1.24192064682e-07 ==== learning rate was 0.0195302799585
iteration #749: max delta: 9.37435687472e-08 ==== learning rate was 0.01951062213
iteration #750: max delta: 0.0194909033356 ==== learning rate was 0.0194909908964
iteration #751: max delta: 0.0194712845601 ==== learning rate was 0.0194713862216
iteration #752: max delta: 0.0194517416336 ==== learning rate was 0.0194518080698
iteration #753: max delta: 0.0194321850727 ==== learning rate was 0.0194322564049
iteration #754: max delta: 0.0194125572055 ==== learning rate was 0.0194127311913
iteration #755: max delta: 0.019393124463 ==== learning rate was 0.0193932323931
iteration #756: max delta: 1.8439703361e-07 ==== learning rate was 0.0193737599745
iteration #757: max delta: 0.0193542484455 ==== learning rate was 0.0193543138999
iteration #758: max delta: 2.08615608782e-07 ==== learning rate was 0.0193348941337
iteration #759: max delta: 0.0193153464591 ==== learning rate was 0.0193155006402
iteration #760: max delta: 1.16220563517e-07 ==== learning rate was 0.0192961333839
iteration #761: max delta: 1.15447770643e-07 ==== learning rate was 0.0192767923294
iteration #762: max delta: 0.0192574121392 ==== learning rate was 0.0192574774411
iteration #763: max delta: 0.0192380618563 ==== learning rate was 0.0192381886836
iteration #764: max delta: 0.019218839335 ==== learning rate was 0.0192189260217
iteration #765: max delta: 0.0191996268868 ==== learning rate was 0.01919968942
iteration #766: max delta: 0.0191803851299 ==== learning rate was 0.0191804788432
iteration #767: max delta: 7.71825867623e-08 ==== learning rate was 0.0191612942562
iteration #768: max delta: 1.17727019855e-07 ==== learning rate was 0.0191421356237
iteration #769: max delta: 0.0191229453506 ==== learning rate was 0.0191230029108
iteration #770: max delta: 1.40583621716e-07 ==== learning rate was 0.0191038960822
iteration #771: max delta: 0.019084769534 ==== learning rate was 0.019084815103
iteration #772: max delta: 8.96671151033e-08 ==== learning rate was 0.0190657599382
iteration #773: max delta: 0.0190466555746 ==== learning rate was 0.0190467305529
iteration #774: max delta: 1.00004071756e-07 ==== learning rate was 0.0190277269122
iteration #775: max delta: 0.0190086737801 ==== learning rate was 0.0190087489813
iteration #776: max delta: 0.0189897050417 ==== learning rate was 0.0189897967254
iteration #777: max delta: 0.018970793496 ==== learning rate was 0.0189708701097
iteration #778: max delta: 6.67918155344e-08 ==== learning rate was 0.0189519690997
iteration #779: max delta: 6.19446431554e-08 ==== learning rate was 0.0189330936605
iteration #780: max delta: 4.54323065764e-08 ==== learning rate was 0.0189142437577
iteration #781: max delta: 0.0188953772723 ==== learning rate was 0.0188954193567
iteration #782: max delta: 8.94249768639e-08 ==== learning rate was 0.018876620423
iteration #783: max delta: 8.75490705956e-08 ==== learning rate was 0.0188578469221
iteration #784: max delta: 1.13434892818e-07 ==== learning rate was 0.0188390988196
iteration #785: max delta: 9.70508950941e-08 ==== learning rate was 0.0188203760812
iteration #786: max delta: 5.64196881566e-08 ==== learning rate was 0.0188016786726
iteration #787: max delta: 4.4471741672e-08 ==== learning rate was 0.0187830065594
iteration #788: max delta: 0.0187643155276 ==== learning rate was 0.0187643597075
iteration #789: max delta: 6.54475147673e-08 ==== learning rate was 0.0187457380827
iteration #790: max delta: 5.29348030351e-08 ==== learning rate was 0.0187271416509
iteration #791: max delta: 0.0187085112524 ==== learning rate was 0.0187085703779
iteration #792: max delta: 0.0186899419856 ==== learning rate was 0.0186900242297
iteration #793: max delta: 3.26815229427e-08 ==== learning rate was 0.0186715031724
iteration #794: max delta: 5.67520527719e-08 ==== learning rate was 0.018653007172
iteration #795: max delta: 9.45330477595e-08 ==== learning rate was 0.0186345361946
iteration #796: max delta: 0.0186160588966 ==== learning rate was 0.0186160902064
iteration #797: max delta: 0.0185976040528 ==== learning rate was 0.0185976691735
iteration #798: max delta: 3.67833541633e-08 ==== learning rate was 0.0185792730621
iteration #799: max delta: 6.60908191693e-08 ==== learning rate was 0.0185609018387
iteration #800: max delta: 0.0185424982531 ==== learning rate was 0.0185425554694
iteration #801: max delta: 5.09301084008e-08 ==== learning rate was 0.0185242339207
iteration #802: max delta: 0.0185058953167 ==== learning rate was 0.0185059371589
iteration #803: max delta: 9.487376684e-08 ==== learning rate was 0.0184876651506
iteration #804: max delta: 1.44118869597e-07 ==== learning rate was 0.0184694178623
iteration #805: max delta: 4.38068535888e-08 ==== learning rate was 0.0184511952605
iteration #806: max delta: 4.48518139147e-08 ==== learning rate was 0.0184329973119
iteration #807: max delta: 6.42441874957e-08 ==== learning rate was 0.018414823983
iteration #808: max delta: 0.0183966354616 ==== learning rate was 0.0183966752405
iteration #809: max delta: 0.0183784938133 ==== learning rate was 0.0183785510513
iteration #810: max delta: 0.0183604129355 ==== learning rate was 0.018360451382
iteration #811: max delta: 0.0183422030874 ==== learning rate was 0.0183423761996
iteration #812: max delta: 6.33346964997e-08 ==== learning rate was 0.0183243254708
iteration #813: max delta: 4.38121742288e-08 ==== learning rate was 0.0183062991627
iteration #814: max delta: 0.0182882630043 ==== learning rate was 0.0182882972421
iteration #815: max delta: 4.10656547843e-08 ==== learning rate was 0.018270319676
iteration #816: max delta: 0.0182523181703 ==== learning rate was 0.0182523664316
iteration #817: max delta: 0.0182343892344 ==== learning rate was 0.0182344374759
iteration #818: max delta: 0.018216484086 ==== learning rate was 0.018216532776
iteration #819: max delta: 0.0181986168694 ==== learning rate was 0.0181986522992
iteration #820: max delta: 0.0181807594718 ==== learning rate was 0.0181807960127
iteration #821: max delta: 3.32350216455e-08 ==== learning rate was 0.0181629638836
iteration #822: max delta: 5.25666824368e-08 ==== learning rate was 0.0181451558795
iteration #823: max delta: 0.0181273435133 ==== learning rate was 0.0181273719676
iteration #824: max delta: 7.76531675202e-08 ==== learning rate was 0.0181096121152
iteration #825: max delta: 5.02154783429e-08 ==== learning rate was 0.01809187629
iteration #826: max delta: 0.0180740995166 ==== learning rate was 0.0180741644593
iteration #827: max delta: 0.0180564327083 ==== learning rate was 0.0180564765908
iteration #828: max delta: 4.40900788858e-08 ==== learning rate was 0.0180388126519
iteration #829: max delta: 0.0180211241921 ==== learning rate was 0.0180211726104
iteration #830: max delta: 1.01293729785e-07 ==== learning rate was 0.0180035564338
iteration #831: max delta: 8.36821005004e-08 ==== learning rate was 0.0179859640899
iteration #832: max delta: 0.0179683572936 ==== learning rate was 0.0179683955465
iteration #833: max delta: 0.0179508324118 ==== learning rate was 0.0179508507714
iteration #834: max delta: 0.0179333032117 ==== learning rate was 0.0179333297323
iteration #835: max delta: 0.0179158059228 ==== learning rate was 0.0179158323972
iteration #836: max delta: 6.0414057463e-08 ==== learning rate was 0.0178983587341
iteration #837: max delta: 3.46405397334e-08 ==== learning rate was 0.0178809087108
iteration #838: max delta: 0.0178634550635 ==== learning rate was 0.0178634822955
iteration #839: max delta: 1.71635779333e-08 ==== learning rate was 0.0178460794561
iteration #840: max delta: 0.0178286583924 ==== learning rate was 0.0178287001608
iteration #841: max delta: 0.0178113077566 ==== learning rate was 0.0178113443777
iteration #842: max delta: 4.14244861571e-08 ==== learning rate was 0.0177940120751
iteration #843: max delta: 4.75249750955e-08 ==== learning rate was 0.017776703221
iteration #844: max delta: 6.28856692885e-08 ==== learning rate was 0.017759417784
iteration #845: max delta: 2.86042243448e-08 ==== learning rate was 0.0177421557321
iteration #846: max delta: 3.86059210515e-08 ==== learning rate was 0.0177249170339
iteration #847: max delta: 2.98095974993e-08 ==== learning rate was 0.0177077016577
iteration #848: max delta: 1.873125234e-07 ==== learning rate was 0.0176905095719
iteration #849: max delta: 5.42321565201e-08 ==== learning rate was 0.0176733407451
iteration #850: max delta: 0.0176561675305 ==== learning rate was 0.0176561951458
iteration #851: max delta: 0.0176390385163 ==== learning rate was 0.0176390727425
iteration #852: max delta: 6.29149812873e-08 ==== learning rate was 0.0176219735039
iteration #853: max delta: 3.95556325529e-08 ==== learning rate was 0.0176048973987
iteration #854: max delta: 2.06276655884e-08 ==== learning rate was 0.0175878443955
iteration #855: max delta: 3.06501237382e-08 ==== learning rate was 0.0175708144631
iteration #856: max delta: 0.0175537817531 ==== learning rate was 0.0175538075702
iteration #857: max delta: 0.0175367974663 ==== learning rate was 0.0175368236858
iteration #858: max delta: 3.8032380065e-08 ==== learning rate was 0.0175198627787
iteration #859: max delta: 0.0175028953862 ==== learning rate was 0.0175029248177
iteration #860: max delta: 4.45272100078e-08 ==== learning rate was 0.0174860097719
iteration #861: max delta: 0.0174690653374 ==== learning rate was 0.0174691176102
iteration #862: max delta: 0.0174522227888 ==== learning rate was 0.0174522483018
iteration #863: max delta: 3.17578380793e-08 ==== learning rate was 0.0174354018155
iteration #864: max delta: 3.06608990018e-08 ==== learning rate was 0.0174185781207
iteration #865: max delta: 0.0174017585386 ==== learning rate was 0.0174017771865
iteration #866: max delta: 2.94525440957e-08 ==== learning rate was 0.017384998982
iteration #867: max delta: 0.0173682233919 ==== learning rate was 0.0173682434765
iteration #868: max delta: 7.21239268777e-08 ==== learning rate was 0.0173515106394
iteration #869: max delta: 2.37812153955e-08 ==== learning rate was 0.0173348004398
iteration #870: max delta: 0.0173180393299 ==== learning rate was 0.0173181128473
iteration #871: max delta: 0.0173014269318 ==== learning rate was 0.0173014478313
iteration #872: max delta: 0.0172847785627 ==== learning rate was 0.0172848053611
iteration #873: max delta: 2.02225951984e-08 ==== learning rate was 0.0172681854063
iteration #874: max delta: 0.0172515503594 ==== learning rate was 0.0172515879364
iteration #875: max delta: 2.60504309217e-08 ==== learning rate was 0.017235012921
iteration #876: max delta: 0.0172184313267 ==== learning rate was 0.0172184603297
iteration #877: max delta: 0.0172019157773 ==== learning rate was 0.0172019301323
iteration #878: max delta: 4.66307288135e-08 ==== learning rate was 0.0171854222983
iteration #879: max delta: 0.0171689097839 ==== learning rate was 0.0171689367975
iteration #880: max delta: 2.70540537445e-08 ==== learning rate was 0.0171524735998
iteration #881: max delta: 3.24158599142e-08 ==== learning rate was 0.0171360326749
iteration #882: max delta: 7.23600907425e-08 ==== learning rate was 0.0171196139928
iteration #883: max delta: 1.86939287987e-08 ==== learning rate was 0.0171032175232
iteration #884: max delta: 0.0170868238013 ==== learning rate was 0.0170868432363
iteration #885: max delta: 0.0170704727836 ==== learning rate was 0.0170704911019
iteration #886: max delta: 1.21744980314e-08 ==== learning rate was 0.0170541610901
iteration #887: max delta: 1.73868978987e-08 ==== learning rate was 0.0170378531709
iteration #888: max delta: 1.44883776909e-08 ==== learning rate was 0.0170215673145
iteration #889: max delta: 2.19397821319e-08 ==== learning rate was 0.0170053034911
iteration #890: max delta: 2.19672187915e-08 ==== learning rate was 0.0169890616707
iteration #891: max delta: 1.91696248817e-08 ==== learning rate was 0.0169728418238
iteration #892: max delta: 0.0169566212736 ==== learning rate was 0.0169566439204
iteration #893: max delta: 0.0169404497485 ==== learning rate was 0.016940467931
iteration #894: max delta: 0.0169242952588 ==== learning rate was 0.0169243138258
iteration #895: max delta: 0.0169081576159 ==== learning rate was 0.0169081815754
iteration #896: max delta: 4.26849123605e-08 ==== learning rate was 0.01689207115
iteration #897: max delta: 1.49205259194e-08 ==== learning rate was 0.0168759825203
iteration #898: max delta: 3.75418822174e-08 ==== learning rate was 0.0168599156566
iteration #899: max delta: 1.22841087102e-08 ==== learning rate was 0.0168438705296
iteration #900: max delta: 1.26760895159e-08 ==== learning rate was 0.0168278471098
iteration #901: max delta: 0.0168118318533 ==== learning rate was 0.016811845368
iteration #902: max delta: 0.0167958477332 ==== learning rate was 0.0167958652746
iteration #903: max delta: 2.76795335021e-08 ==== learning rate was 0.0167799068006
iteration #904: max delta: 0.0167639578364 ==== learning rate was 0.0167639699165
iteration #905: max delta: 0.0167480410977 ==== learning rate was 0.0167480545933
iteration #906: max delta: 2.00514027067e-08 ==== learning rate was 0.0167321608016
iteration #907: max delta: 0.0167162803634 ==== learning rate was 0.0167162885125
iteration #908: max delta: 1.71243789744e-08 ==== learning rate was 0.0167004376968
iteration #909: max delta: 0.0166845887104 ==== learning rate was 0.0166846083255
iteration #910: max delta: 0.0166687892865 ==== learning rate was 0.0166688003695
iteration #911: max delta: 1.66884830525e-08 ==== learning rate was 0.0166530137999
iteration #912: max delta: 1.69147255004e-08 ==== learning rate was 0.0166372485878
iteration #913: max delta: 0.0166214724501 ==== learning rate was 0.0166215047042
iteration #914: max delta: 2.60518510264e-08 ==== learning rate was 0.0166057821203
iteration #915: max delta: 0.01659007371 ==== learning rate was 0.0165900808073
iteration #916: max delta: 1.56416641441e-08 ==== learning rate was 0.0165744007363
iteration #917: max delta: 1.03446612939e-08 ==== learning rate was 0.0165587418788
iteration #918: max delta: 2.14474093196e-08 ==== learning rate was 0.0165431042059
iteration #919: max delta: 0.0165274703416 ==== learning rate was 0.016527487689
iteration #920: max delta: 0.0165118648572 ==== learning rate was 0.0165118922995
iteration #921: max delta: 4.07478792356e-08 ==== learning rate was 0.0164963180088
iteration #922: max delta: 0.0164807466563 ==== learning rate was 0.0164807647884
iteration #923: max delta: 6.66268239805e-09 ==== learning rate was 0.0164652326097
iteration #924: max delta: 0.0164496896454 ==== learning rate was 0.0164497214443
iteration #925: max delta: 0.0164342139581 ==== learning rate was 0.0164342312638
iteration #926: max delta: 1.57736206811e-08 ==== learning rate was 0.0164187620397
iteration #927: max delta: 2.61648557795e-08 ==== learning rate was 0.0164033137437
iteration #928: max delta: 0.0163878689368 ==== learning rate was 0.0163878863476
iteration #929: max delta: 1.13754566501e-08 ==== learning rate was 0.0163724798229
iteration #930: max delta: 0.0163570778988 ==== learning rate was 0.0163570941416
iteration #931: max delta: 0.0163417149224 ==== learning rate was 0.0163417292753
iteration #932: max delta: 0.0163263797971 ==== learning rate was 0.016326385196
iteration #933: max delta: 0.0163110523902 ==== learning rate was 0.0163110618755
iteration #934: max delta: 0.0162957504426 ==== learning rate was 0.0162957592857
iteration #935: max delta: 2.50288050656e-08 ==== learning rate was 0.0162804773985
iteration #936: max delta: 0.0162652060894 ==== learning rate was 0.0162652161861
iteration #937: max delta: 1.37014703226e-08 ==== learning rate was 0.0162499756203
iteration #938: max delta: 1.22402202289e-08 ==== learning rate was 0.0162347556733
iteration #939: max delta: 1.39006579858e-08 ==== learning rate was 0.0162195563172
iteration #940: max delta: 1.00312409214e-08 ==== learning rate was 0.0162043775241
iteration #941: max delta: 0.0161892137409 ==== learning rate was 0.0161892192662
iteration #942: max delta: 0.0161740775762 ==== learning rate was 0.0161740815157
iteration #943: max delta: 0.0161589563912 ==== learning rate was 0.0161589642448
iteration #944: max delta: 0.0161438513744 ==== learning rate was 0.016143867426
iteration #945: max delta: 0.0161287811316 ==== learning rate was 0.0161287910314
iteration #946: max delta: 8.28041893379e-09 ==== learning rate was 0.0161137350334
iteration #947: max delta: 0.016098685531 ==== learning rate was 0.0160986994046
iteration #948: max delta: 2.54934193634e-08 ==== learning rate was 0.0160836841172
iteration #949: max delta: 0.0160686816506 ==== learning rate was 0.0160686891439
iteration #950: max delta: 2.80316760638e-08 ==== learning rate was 0.016053714457
iteration #951: max delta: 0.0160387515403 ==== learning rate was 0.0160387600292
iteration #952: max delta: 1.69050173181e-08 ==== learning rate was 0.0160238258331
iteration #953: max delta: 0.0160089054887 ==== learning rate was 0.0160089118412
iteration #954: max delta: 0.0159940103005 ==== learning rate was 0.0159940180263
iteration #955: max delta: 1.04616216119e-08 ==== learning rate was 0.015979144361
iteration #956: max delta: 9.92972035523e-09 ==== learning rate was 0.0159642908182
iteration #957: max delta: 7.75591460908e-09 ==== learning rate was 0.0159494573705
iteration #958: max delta: 0.015934638661 ==== learning rate was 0.0159346439907
iteration #959: max delta: 0.0159198351423 ==== learning rate was 0.0159198506518
iteration #960: max delta: 8.28425372544e-09 ==== learning rate was 0.0159050773267
iteration #961: max delta: 0.0158903164269 ==== learning rate was 0.0158903239881
iteration #962: max delta: 0.0158755836394 ==== learning rate was 0.0158755906092
iteration #963: max delta: 0.0158608713082 ==== learning rate was 0.0158608771628
iteration #964: max delta: 0.0158461770326 ==== learning rate was 0.0158461836221
iteration #965: max delta: 9.49382486772e-09 ==== learning rate was 0.0158315099601
iteration #966: max delta: 6.86081662053e-09 ==== learning rate was 0.0158168561499
iteration #967: max delta: 0.0158022143943 ==== learning rate was 0.0158022221647
iteration #968: max delta: 1.12833831972e-08 ==== learning rate was 0.0157876079776
iteration #969: max delta: 0.0157730071511 ==== learning rate was 0.0157730135618
iteration #970: max delta: 7.24720074401e-09 ==== learning rate was 0.0157584388906
iteration #971: max delta: 0.015743877883 ==== learning rate was 0.0157438839373
iteration #972: max delta: 0.015729341754 ==== learning rate was 0.0157293486753
iteration #973: max delta: 0.01571482837 ==== learning rate was 0.0157148330777
iteration #974: max delta: 8.73516162179e-09 ==== learning rate was 0.0157003371182
iteration #975: max delta: 0.0156858510994 ==== learning rate was 0.0156858607701
iteration #976: max delta: 1.06696188011e-08 ==== learning rate was 0.0156714040068
iteration #977: max delta: 6.80420225932e-09 ==== learning rate was 0.0156569668019
iteration #978: max delta: 0.0156425423456 ==== learning rate was 0.0156425491288
iteration #979: max delta: 6.00851673758e-09 ==== learning rate was 0.0156281509613
iteration #980: max delta: 6.15809506425e-09 ==== learning rate was 0.0156137722729
iteration #981: max delta: 0.0155994095664 ==== learning rate was 0.0155994130372
iteration #982: max delta: 0.0155850678475 ==== learning rate was 0.0155850732279
iteration #983: max delta: 0.0155707483894 ==== learning rate was 0.0155707528188
iteration #984: max delta: 0.0155564449475 ==== learning rate was 0.0155564517836
iteration #985: max delta: 5.19179246462e-09 ==== learning rate was 0.0155421700961
iteration #986: max delta: 0.0155279027585 ==== learning rate was 0.01552790773
iteration #987: max delta: 0.0155136529454 ==== learning rate was 0.0155136646594
iteration #988: max delta: 5.90920237696e-09 ==== learning rate was 0.015499440858
iteration #989: max delta: 0.0154852309111 ==== learning rate was 0.0154852362998
iteration #990: max delta: 5.45208641507e-09 ==== learning rate was 0.0154710509588
iteration #991: max delta: 0.0154568797599 ==== learning rate was 0.0154568848089
iteration #992: max delta: 1.00557843649e-08 ==== learning rate was 0.0154427378243
iteration #993: max delta: 2.54660742201e-09 ==== learning rate was 0.0154286099789
iteration #994: max delta: 0.0154144942257 ==== learning rate was 0.0154145012469
iteration #995: max delta: 0.0154004004474 ==== learning rate was 0.0154004116024
iteration #996: max delta: 0.0153863328239 ==== learning rate was 0.0153863410196
iteration #997: max delta: 0.0153722748232 ==== learning rate was 0.0153722894727
iteration #998: max delta: 4.90266359348e-09 ==== learning rate was 0.015358256936
iteration #999: max delta: 7.53964677558e-09 ==== learning rate was 0.0153442433837
iteration #1000: max delta: 0.0153302440332 ==== learning rate was 0.0153302487902
iteration #1001: max delta: 4.33783675188e-09 ==== learning rate was 0.0153162731298
iteration #1002: max delta: 0.0153023124697 ==== learning rate was 0.0153023163769
iteration #1003: max delta: 6.502571184e-09 ==== learning rate was 0.0152883785058
iteration #1004: max delta: 4.41150778874e-09 ==== learning rate was 0.0152744594912
iteration #1005: max delta: 0.0152605566361 ==== learning rate was 0.0152605593074
iteration #1006: max delta: 0.0152466718277 ==== learning rate was 0.015246677929
iteration #1007: max delta: 1.25007261091e-08 ==== learning rate was 0.0152328153305
iteration #1008: max delta: 1.76317111703e-08 ==== learning rate was 0.0152189714865
iteration #1009: max delta: 5.49910548292e-09 ==== learning rate was 0.0152051463717
iteration #1010: max delta: 0.0151913381796 ==== learning rate was 0.0151913399608
iteration #1011: max delta: 0.0151775494127 ==== learning rate was 0.0151775522283
iteration #1012: max delta: 5.30460789672e-09 ==== learning rate was 0.0151637831491
iteration #1013: max delta: 4.1648973873e-09 ==== learning rate was 0.0151500326979
iteration #1014: max delta: 5.51041822003e-09 ==== learning rate was 0.0151363008495
iteration #1015: max delta: 0.0151225840048 ==== learning rate was 0.0151225875788
iteration #1016: max delta: 0.0151088897342 ==== learning rate was 0.0151088928605
iteration #1017: max delta: 1.69232880162e-08 ==== learning rate was 0.0150952166697
iteration #1018: max delta: 0.0150815549404 ==== learning rate was 0.0150815589812
iteration #1019: max delta: 0.0150679163777 ==== learning rate was 0.01506791977
iteration #1020: max delta: 0.0150542962258 ==== learning rate was 0.0150542990111
iteration #1021: max delta: 6.54320463159e-09 ==== learning rate was 0.0150406966796
iteration #1022: max delta: 6.73132336651e-09 ==== learning rate was 0.0150271127505
iteration #1023: max delta: 4.00836358548e-09 ==== learning rate was 0.0150135471989
iteration #1024: max delta: 5.34826815282e-09 ==== learning rate was 0.015
iteration #1025: max delta: 0.0149864699575 ==== learning rate was 0.0149864711289
iteration #1026: max delta: 6.1246506115e-09 ==== learning rate was 0.0149729605609
iteration #1027: max delta: 0.014959461609 ==== learning rate was 0.0149594682711
iteration #1028: max delta: 4.87154075847e-09 ==== learning rate was 0.0149459942348
iteration #1029: max delta: 1.96055842973e-09 ==== learning rate was 0.0149325384274
iteration #1030: max delta: 6.5396164216e-09 ==== learning rate was 0.0149191008243
iteration #1031: max delta: 0.0149056784327 ==== learning rate was 0.0149056814006
iteration #1032: max delta: 0.0148922787724 ==== learning rate was 0.0148922801319
iteration #1033: max delta: 2.85323515135e-09 ==== learning rate was 0.0148788969937
iteration #1034: max delta: 1.64357400018e-08 ==== learning rate was 0.0148655319613
iteration #1035: max delta: 0.0148521819751 ==== learning rate was 0.0148521850103
iteration #1036: max delta: 0.0148388503389 ==== learning rate was 0.0148388561162
iteration #1037: max delta: 0.0148255375991 ==== learning rate was 0.0148255452546
iteration #1038: max delta: 0.0148122487533 ==== learning rate was 0.014812252401
iteration #1039: max delta: 3.09211008337e-09 ==== learning rate was 0.0147989775313
iteration #1040: max delta: 0.0147857170824 ==== learning rate was 0.0147857206209
iteration #1041: max delta: 0.0147724797315 ==== learning rate was 0.0147724816456
iteration #1042: max delta: 0.0147592584039 ==== learning rate was 0.0147592605812
iteration #1043: max delta: 1.73103681742e-09 ==== learning rate was 0.0147460574033
iteration #1044: max delta: 2.10079783076e-09 ==== learning rate was 0.0147328720879
iteration #1045: max delta: 2.56152476597e-09 ==== learning rate was 0.0147197046107
iteration #1046: max delta: 5.0831075972e-09 ==== learning rate was 0.0147065549476
iteration #1047: max delta: 0.0146934162195 ==== learning rate was 0.0146934230746
iteration #1048: max delta: 0.014680307027 ==== learning rate was 0.0146803089675
iteration #1049: max delta: 5.62576186359e-09 ==== learning rate was 0.0146672126022
iteration #1050: max delta: 0.0146541314181 ==== learning rate was 0.0146541339549
iteration #1051: max delta: 0.0146410710225 ==== learning rate was 0.0146410730016
iteration #1052: max delta: 0.0146280224555 ==== learning rate was 0.0146280297182
iteration #1053: max delta: 0.0146150003593 ==== learning rate was 0.0146150040809
iteration #1054: max delta: 4.84053653097e-09 ==== learning rate was 0.0146019960658
iteration #1055: max delta: 0.0145890040714 ==== learning rate was 0.0145890056491
iteration #1056: max delta: 0.0145760266068 ==== learning rate was 0.014576032807
iteration #1057: max delta: 0.0145630720193 ==== learning rate was 0.0145630775156
iteration #1058: max delta: 0.0145501386034 ==== learning rate was 0.0145501397514
iteration #1059: max delta: 5.58981765484e-09 ==== learning rate was 0.0145372194904
iteration #1060: max delta: 4.33444855871e-09 ==== learning rate was 0.0145243167091
iteration #1061: max delta: 1.64773606148e-09 ==== learning rate was 0.0145114313838
iteration #1062: max delta: 1.97429228468e-09 ==== learning rate was 0.0144985634909
iteration #1063: max delta: 0.0144857114574 ==== learning rate was 0.0144857130068
iteration #1064: max delta: 0.0144728773866 ==== learning rate was 0.0144728799079
iteration #1065: max delta: 0.0144600626752 ==== learning rate was 0.0144600641708
iteration #1066: max delta: 0.0144472637455 ==== learning rate was 0.014447265772
iteration #1067: max delta: 4.65197003993e-09 ==== learning rate was 0.0144344846879
iteration #1068: max delta: 2.49006100963e-09 ==== learning rate was 0.0144217208952
iteration #1069: max delta: 0.0144089719491 ==== learning rate was 0.0144089743704
iteration #1070: max delta: 0.01439624214 ==== learning rate was 0.0143962450903
iteration #1071: max delta: 0.0143835306837 ==== learning rate was 0.0143835330314
iteration #1072: max delta: 3.81369803155e-09 ==== learning rate was 0.0143708381706
iteration #1073: max delta: 3.00301970247e-09 ==== learning rate was 0.0143581604844
iteration #1074: max delta: 2.55390910459e-09 ==== learning rate was 0.0143454999497
iteration #1075: max delta: 0.0143328552188 ==== learning rate was 0.0143328565433
iteration #1076: max delta: 2.11097700209e-09 ==== learning rate was 0.014320230242
iteration #1077: max delta: 4.55610342264e-09 ==== learning rate was 0.0143076210226
iteration #1078: max delta: 0.0142950219867 ==== learning rate was 0.0142950288621
iteration #1079: max delta: 7.08674903976e-09 ==== learning rate was 0.0142824537374
iteration #1080: max delta: 1.90102322749e-09 ==== learning rate was 0.0142698956254
iteration #1081: max delta: 0.0142573518098 ==== learning rate was 0.0142573545031
iteration #1082: max delta: 0.0142448295235 ==== learning rate was 0.0142448303476
iteration #1083: max delta: 0.0142323218756 ==== learning rate was 0.0142323231357
iteration #1084: max delta: 0.0142198310786 ==== learning rate was 0.0142198328448
iteration #1085: max delta: 1.59964633676e-09 ==== learning rate was 0.0142073594518
iteration #1086: max delta: 3.20983141842e-09 ==== learning rate was 0.0141949029339
iteration #1087: max delta: 2.4245079207e-09 ==== learning rate was 0.0141824632682
iteration #1088: max delta: 1.67512343678e-09 ==== learning rate was 0.014170040432
iteration #1089: max delta: 0.0141576332428 ==== learning rate was 0.0141576344026
iteration #1090: max delta: 0.0141452426161 ==== learning rate was 0.014145245157
iteration #1091: max delta: 0.0141328687108 ==== learning rate was 0.0141328726727
iteration #1092: max delta: 0.0141205155484 ==== learning rate was 0.014120516927
iteration #1093: max delta: 3.420166818e-09 ==== learning rate was 0.0141081778973
iteration #1094: max delta: 3.59611693055e-09 ==== learning rate was 0.0140958555608
iteration #1095: max delta: 0.0140835486153 ==== learning rate was 0.0140835498951
iteration #1096: max delta: 2.40504434445e-09 ==== learning rate was 0.0140712608775
iteration #1097: max delta: 8.03884276633e-10 ==== learning rate was 0.0140589884856
iteration #1098: max delta: 0.0140467316644 ==== learning rate was 0.0140467326969
iteration #1099: max delta: 0.0140344898116 ==== learning rate was 0.0140344934888
iteration #1100: max delta: 0.0140222697124 ==== learning rate was 0.014022270839
iteration #1101: max delta: 0.0140100639123 ==== learning rate was 0.0140100647251
iteration #1102: max delta: 2.59564107372e-09 ==== learning rate was 0.0139978751247
iteration #1103: max delta: 0.0139856999302 ==== learning rate was 0.0139857020154
iteration #1104: max delta: 0.0139735439714 ==== learning rate was 0.013973545375
iteration #1105: max delta: 1.0702928631e-09 ==== learning rate was 0.0139614051812
iteration #1106: max delta: 0.01394927908 ==== learning rate was 0.0139492814116
iteration #1107: max delta: 0.0139371706308 ==== learning rate was 0.0139371740442
iteration #1108: max delta: 4.49902154014e-09 ==== learning rate was 0.0139250830566
iteration #1109: max delta: 4.68269213237e-09 ==== learning rate was 0.0139130084268
iteration #1110: max delta: 1.17548174443e-09 ==== learning rate was 0.0139009501326
iteration #1111: max delta: 2.06693262394e-09 ==== learning rate was 0.0138889081519
iteration #1112: max delta: 9.64548019983e-10 ==== learning rate was 0.0138768824626
iteration #1113: max delta: 0.0138648717416 ==== learning rate was 0.0138648730428
iteration #1114: max delta: 2.00005107457e-09 ==== learning rate was 0.0138528798703
iteration #1115: max delta: 2.37565219861e-09 ==== learning rate was 0.0138409029233
iteration #1116: max delta: 0.0138289403256 ==== learning rate was 0.0138289421797
iteration #1117: max delta: 0.0138169920089 ==== learning rate was 0.0138169976176
iteration #1118: max delta: 3.28230794322e-09 ==== learning rate was 0.0138050692152
iteration #1119: max delta: 0.0137931562226 ==== learning rate was 0.0137931569506
iteration #1120: max delta: 1.85529781123e-09 ==== learning rate was 0.0137812608019
iteration #1121: max delta: 0.0137693765194 ==== learning rate was 0.0137693807473
iteration #1122: max delta: 1.38541506632e-09 ==== learning rate was 0.0137575167652
iteration #1123: max delta: 1.80824342471e-09 ==== learning rate was 0.0137456688336
iteration #1124: max delta: 0.0137338359232 ==== learning rate was 0.013733836931
iteration #1125: max delta: 0.0137220202051 ==== learning rate was 0.0137220210356
iteration #1126: max delta: 2.19676888128e-09 ==== learning rate was 0.0137102211258
iteration #1127: max delta: 1.15420335752e-09 ==== learning rate was 0.0136984371799
iteration #1128: max delta: 2.14146553102e-09 ==== learning rate was 0.0136866691764
iteration #1129: max delta: 0.0136749162563 ==== learning rate was 0.0136749170936
iteration #1130: max delta: 7.31460870035e-10 ==== learning rate was 0.0136631809101
iteration #1131: max delta: 3.12697489144e-09 ==== learning rate was 0.0136514606043
iteration #1132: max delta: 0.013639754811 ==== learning rate was 0.0136397561548
iteration #1133: max delta: 0.0136280665106 ==== learning rate was 0.0136280675401
iteration #1134: max delta: 0.0136163940986 ==== learning rate was 0.0136163947387
iteration #1135: max delta: 7.66899265201e-10 ==== learning rate was 0.0136047377294
iteration #1136: max delta: 5.94018885239e-10 ==== learning rate was 0.0135930964906
iteration #1137: max delta: 2.7490498865e-09 ==== learning rate was 0.0135814710011
iteration #1138: max delta: 2.91325511007e-09 ==== learning rate was 0.0135698612396
iteration #1139: max delta: 0.0135582649364 ==== learning rate was 0.0135582671849
iteration #1140: max delta: 0.0135466878184 ==== learning rate was 0.0135466888155
iteration #1141: max delta: 8.62743144594e-10 ==== learning rate was 0.0135351261104
iteration #1142: max delta: 6.5889518585e-10 ==== learning rate was 0.0135235790483
iteration #1143: max delta: 7.17070703181e-10 ==== learning rate was 0.0135120476081
iteration #1144: max delta: 9.18974465362e-10 ==== learning rate was 0.0135005317686
iteration #1145: max delta: 0.0134890309874 ==== learning rate was 0.0134890315087
iteration #1146: max delta: 0.0134775459695 ==== learning rate was 0.0134775468074
iteration #1147: max delta: 8.38072826449e-10 ==== learning rate was 0.0134660776437
iteration #1148: max delta: 1.43472684664e-09 ==== learning rate was 0.0134546239963
iteration #1149: max delta: 0.0134431847733 ==== learning rate was 0.0134431858445
iteration #1150: max delta: 4.60642212097e-09 ==== learning rate was 0.0134317631672
iteration #1151: max delta: 0.0134203538685 ==== learning rate was 0.0134203559435
iteration #1152: max delta: 1.14991643011e-09 ==== learning rate was 0.0134089641525
iteration #1153: max delta: 0.0133975867178 ==== learning rate was 0.0133975877733
iteration #1154: max delta: 1.43101947307e-09 ==== learning rate was 0.0133862267851
iteration #1155: max delta: 3.44971983983e-09 ==== learning rate was 0.013374881167
iteration #1156: max delta: 2.20886374804e-09 ==== learning rate was 0.0133635508982
iteration #1157: max delta: 0.0133522312293 ==== learning rate was 0.013352235958
iteration #1158: max delta: 0.0133409353896 ==== learning rate was 0.0133409363257
iteration #1159: max delta: 0.0133296510645 ==== learning rate was 0.0133296519804
iteration #1160: max delta: 0.0133183825643 ==== learning rate was 0.0133183829016
iteration #1161: max delta: 1.58079370215e-09 ==== learning rate was 0.0133071290686
iteration #1162: max delta: 0.0132958876975 ==== learning rate was 0.0132958904608
iteration #1163: max delta: 1.4923877906e-09 ==== learning rate was 0.0132846670575
iteration #1164: max delta: 4.04543726397e-09 ==== learning rate was 0.0132734588383
iteration #1165: max delta: 2.57057597624e-09 ==== learning rate was 0.0132622657825
iteration #1166: max delta: 0.0132510861644 ==== learning rate was 0.0132510878696
iteration #1167: max delta: 0.013239923687 ==== learning rate was 0.0132399250792
iteration #1168: max delta: 0.0132287758179 ==== learning rate was 0.0132287773908
iteration #1169: max delta: 0.0132176443494 ==== learning rate was 0.0132176447839
iteration #1170: max delta: 0.0132065252893 ==== learning rate was 0.0132065272382
iteration #1171: max delta: 0.0131954236019 ==== learning rate was 0.0131954247333
iteration #1172: max delta: 5.05481040662e-10 ==== learning rate was 0.0131843372488
iteration #1173: max delta: 0.0131732641088 ==== learning rate was 0.0131732647645
iteration #1174: max delta: 0.0131622063613 ==== learning rate was 0.0131622072599
iteration #1175: max delta: 8.75407108001e-10 ==== learning rate was 0.013151164715
iteration #1176: max delta: 2.53249190966e-09 ==== learning rate was 0.0131401371093
iteration #1177: max delta: 0.0131291240443 ==== learning rate was 0.0131291244227
iteration #1178: max delta: 2.33176128234e-09 ==== learning rate was 0.0131181266351
iteration #1179: max delta: 1.43274878699e-09 ==== learning rate was 0.0131071437262
iteration #1180: max delta: 4.62138697347e-10 ==== learning rate was 0.013096175676
iteration #1181: max delta: 6.51899592062e-10 ==== learning rate was 0.0130852224643
iteration #1182: max delta: 0.0130742835674 ==== learning rate was 0.013074284071
iteration #1183: max delta: 0.013063360098 ==== learning rate was 0.0130633604762
iteration #1184: max delta: 0.0130524497296 ==== learning rate was 0.0130524516597
iteration #1185: max delta: 7.3321649217e-10 ==== learning rate was 0.0130415576017
iteration #1186: max delta: 9.15476447216e-10 ==== learning rate was 0.0130306782821
iteration #1187: max delta: 0.0130198132573 ==== learning rate was 0.013019813681
iteration #1188: max delta: 0.0130089629336 ==== learning rate was 0.0130089637784
iteration #1189: max delta: 0.0129981281198 ==== learning rate was 0.0129981285546
iteration #1190: max delta: 0.0129873071006 ==== learning rate was 0.0129873079895
iteration #1191: max delta: 0.0129765014038 ==== learning rate was 0.0129765020635
iteration #1192: max delta: 0.0129657097432 ==== learning rate was 0.0129657107567
iteration #1193: max delta: 0.0129549330611 ==== learning rate was 0.0129549340493
iteration #1194: max delta: 0.0129441704474 ==== learning rate was 0.0129441719216
iteration #1195: max delta: 1.53024226689e-09 ==== learning rate was 0.0129334243538
iteration #1196: max delta: 5.74314460483e-10 ==== learning rate was 0.0129226913263
iteration #1197: max delta: 0.0129119722364 ==== learning rate was 0.0129119728193
iteration #1198: max delta: 7.80789554548e-10 ==== learning rate was 0.0129012688133
iteration #1199: max delta: 4.54828242502e-10 ==== learning rate was 0.0128905792885
iteration #1200: max delta: 3.17044933487e-10 ==== learning rate was 0.0128799042255
iteration #1201: max delta: 0.0128692431645 ==== learning rate was 0.0128692436047
iteration #1202: max delta: 5.51497231513e-10 ==== learning rate was 0.0128585974065
iteration #1203: max delta: 7.27300192037e-10 ==== learning rate was 0.0128479656113
iteration #1204: max delta: 1.00835117482e-09 ==== learning rate was 0.0128373481998
iteration #1205: max delta: 1.87289093787e-09 ==== learning rate was 0.0128267451525
iteration #1206: max delta: 0.0128161558465 ==== learning rate was 0.0128161564499
iteration #1207: max delta: 0.012805581243 ==== learning rate was 0.0128055820726
iteration #1208: max delta: 2.98258183297e-10 ==== learning rate was 0.0127950220012
iteration #1209: max delta: 0.0127844733723 ==== learning rate was 0.0127844762164
iteration #1210: max delta: 1.36449307532e-09 ==== learning rate was 0.0127739446989
iteration #1211: max delta: 1.67548765541e-09 ==== learning rate was 0.0127634274293
iteration #1212: max delta: 0.0127529238093 ==== learning rate was 0.0127529243884
iteration #1213: max delta: 0.0127424342178 ==== learning rate was 0.012742435557
iteration #1214: max delta: 3.43498620878e-10 ==== learning rate was 0.0127319609157
iteration #1215: max delta: 0.0127214998768 ==== learning rate was 0.0127215004455
iteration #1216: max delta: 6.99278569951e-10 ==== learning rate was 0.012711054127
iteration #1217: max delta: 0.0127006212141 ==== learning rate was 0.0127006219413
iteration #1218: max delta: 0.0126902035339 ==== learning rate was 0.0126902038692
iteration #1219: max delta: 0.0126797996268 ==== learning rate was 0.0126797998915
iteration #1220: max delta: 5.57871596876e-10 ==== learning rate was 0.0126694099892
iteration #1221: max delta: 0.0126590336375 ==== learning rate was 0.0126590341433
iteration #1222: max delta: 0.0126486705594 ==== learning rate was 0.0126486723347
iteration #1223: max delta: 0.0126383242532 ==== learning rate was 0.0126383245445
iteration #1224: max delta: 2.97145994804e-10 ==== learning rate was 0.0126279907537
iteration #1225: max delta: 0.0126176707393 ==== learning rate was 0.0126176709434
iteration #1226: max delta: 0.0126073647456 ==== learning rate was 0.0126073650945
iteration #1227: max delta: 0.0125970720816 ==== learning rate was 0.0125970731884
iteration #1228: max delta: 0.0125867950006 ==== learning rate was 0.012586795206
iteration #1229: max delta: 9.52641853739e-10 ==== learning rate was 0.0125765311286
iteration #1230: max delta: 6.48253949722e-10 ==== learning rate was 0.0125662809373
iteration #1231: max delta: 0.0125560438355 ==== learning rate was 0.0125560446133
iteration #1232: max delta: 1.09821428239e-09 ==== learning rate was 0.012545822138
iteration #1233: max delta: 0.0125356118311 ==== learning rate was 0.0125356134925
iteration #1234: max delta: 0.0125254184769 ==== learning rate was 0.0125254186581
iteration #1235: max delta: 0.0125152372845 ==== learning rate was 0.0125152376162
iteration #1236: max delta: 1.27652762085e-09 ==== learning rate was 0.0125050703481
iteration #1237: max delta: 1.96737482347e-10 ==== learning rate was 0.0124949168352
iteration #1238: max delta: 2.32308872496e-10 ==== learning rate was 0.0124847770588
iteration #1239: max delta: 2.43624815921e-10 ==== learning rate was 0.0124746510004
iteration #1240: max delta: 7.99802375319e-10 ==== learning rate was 0.0124645386415
iteration #1241: max delta: 0.0124544397616 ==== learning rate was 0.0124544399634
iteration #1242: max delta: 0.0124443540338 ==== learning rate was 0.0124443549476
iteration #1243: max delta: 0.0124342833283 ==== learning rate was 0.0124342835758
iteration #1244: max delta: 0.0124242254141 ==== learning rate was 0.0124242258294
iteration #1245: max delta: 3.21257009975e-10 ==== learning rate was 0.0124141816899
iteration #1246: max delta: 2.4550340085e-10 ==== learning rate was 0.0124041511391
iteration #1247: max delta: 7.99824638149e-10 ==== learning rate was 0.0123941341585
iteration #1248: max delta: 3.39502631549e-10 ==== learning rate was 0.0123841307297
iteration #1249: max delta: 3.16321185055e-10 ==== learning rate was 0.0123741408344
iteration #1250: max delta: 3.0545125116e-10 ==== learning rate was 0.0123641644543
iteration #1251: max delta: 0.0123542013227 ==== learning rate was 0.0123542015712
iteration #1252: max delta: 0.0123442520436 ==== learning rate was 0.0123442521667
iteration #1253: max delta: 0.0123343160748 ==== learning rate was 0.0123343162226
iteration #1254: max delta: 7.29678700697e-10 ==== learning rate was 0.0123243937207
iteration #1255: max delta: 3.07811187878e-10 ==== learning rate was 0.0123144846429
iteration #1256: max delta: 0.0123045872438 ==== learning rate was 0.0123045889709
iteration #1257: max delta: 0.0122947065086 ==== learning rate was 0.0122947066866
iteration #1258: max delta: 5.56360203738e-10 ==== learning rate was 0.012284837772
iteration #1259: max delta: 0.01227498163 ==== learning rate was 0.0122749822089
iteration #1260: max delta: 3.8205369393e-10 ==== learning rate was 0.0122651399792
iteration #1261: max delta: 8.0211520552e-10 ==== learning rate was 0.012255311065
iteration #1262: max delta: 0.0122454949087 ==== learning rate was 0.0122454954482
iteration #1263: max delta: 0.0122356928391 ==== learning rate was 0.0122356931108
iteration #1264: max delta: 0.0122259033133 ==== learning rate was 0.0122259040349
iteration #1265: max delta: 7.34641639712e-10 ==== learning rate was 0.0122161282025
iteration #1266: max delta: 0.0122063648834 ==== learning rate was 0.0122063655956
iteration #1267: max delta: 0.0121966155478 ==== learning rate was 0.0121966161965
iteration #1268: max delta: 0.0121868794323 ==== learning rate was 0.0121868799872
iteration #1269: max delta: 0.0121771563907 ==== learning rate was 0.01217715695
iteration #1270: max delta: 0.012167446627 ==== learning rate was 0.0121674470668
iteration #1271: max delta: 4.72444764966e-10 ==== learning rate was 0.0121577503201
iteration #1272: max delta: 0.0121480665307 ==== learning rate was 0.012148066692
iteration #1273: max delta: 5.28774344338e-10 ==== learning rate was 0.0121383961647
iteration #1274: max delta: 0.0121287383349 ==== learning rate was 0.0121287387205
iteration #1275: max delta: 3.04632701846e-10 ==== learning rate was 0.0121190943418
iteration #1276: max delta: 3.01426485705e-10 ==== learning rate was 0.0121094630108
iteration #1277: max delta: 0.0120998438894 ==== learning rate was 0.01209984471
iteration #1278: max delta: 0.0120902392241 ==== learning rate was 0.0120902394216
iteration #1279: max delta: 0.0120806468574 ==== learning rate was 0.0120806471281
iteration #1280: max delta: 0.0120710675863 ==== learning rate was 0.0120710678119
iteration #1281: max delta: 0.0120615009045 ==== learning rate was 0.0120615014554
iteration #1282: max delta: 0.012051947623 ==== learning rate was 0.0120519480411
iteration #1283: max delta: 0.0120424071724 ==== learning rate was 0.0120424075515
iteration #1284: max delta: 4.03922078055e-10 ==== learning rate was 0.0120328799691
iteration #1285: max delta: 9.51676475231e-10 ==== learning rate was 0.0120233652764
iteration #1286: max delta: 1.20857883716e-09 ==== learning rate was 0.0120138634561
iteration #1287: max delta: 0.012004374183 ==== learning rate was 0.0120043744906
iteration #1288: max delta: 4.49805576307e-10 ==== learning rate was 0.0119948983627
iteration #1289: max delta: 2.46418954063e-10 ==== learning rate was 0.0119854350549
iteration #1290: max delta: 0.011975981447 ==== learning rate was 0.0119759845498
iteration #1291: max delta: 4.60866102784e-10 ==== learning rate was 0.0119665468303
iteration #1292: max delta: 4.80710468297e-10 ==== learning rate was 0.0119571218789
iteration #1293: max delta: 1.05995016473e-10 ==== learning rate was 0.0119477096784
iteration #1294: max delta: 3.26748108455e-10 ==== learning rate was 0.0119383102115
iteration #1295: max delta: 0.0119289230365 ==== learning rate was 0.011928923461
iteration #1296: max delta: 3.68197479528e-10 ==== learning rate was 0.0119195494098
iteration #1297: max delta: 0.011910187543 ==== learning rate was 0.0119101880406
iteration #1298: max delta: 2.33612210643e-10 ==== learning rate was 0.0119008393363
iteration #1299: max delta: 1.18995872501e-10 ==== learning rate was 0.0118915032797
iteration #1300: max delta: 3.5165724435e-10 ==== learning rate was 0.0118821798538
iteration #1301: max delta: 3.87438970396e-10 ==== learning rate was 0.0118728690414
iteration #1302: max delta: 0.0118635707361 ==== learning rate was 0.0118635708254
iteration #1303: max delta: 2.7941677072e-10 ==== learning rate was 0.0118542851889
iteration #1304: max delta: 5.64309045563e-10 ==== learning rate was 0.0118450121149
iteration #1305: max delta: 7.86786338263e-10 ==== learning rate was 0.0118357515862
iteration #1306: max delta: 1.67929517106e-10 ==== learning rate was 0.011826503586
iteration #1307: max delta: 1.66954855566e-10 ==== learning rate was 0.0118172680973
iteration #1308: max delta: 0.0118080447543 ==== learning rate was 0.0118080451032
iteration #1309: max delta: 0.0117988343933 ==== learning rate was 0.0117988345867
iteration #1310: max delta: 0.0117896363595 ==== learning rate was 0.0117896365311
iteration #1311: max delta: 3.31881448521e-10 ==== learning rate was 0.0117804509193
iteration #1312: max delta: 0.0117712775058 ==== learning rate was 0.0117712777347
iteration #1313: max delta: 2.06457117121e-10 ==== learning rate was 0.0117621169603
iteration #1314: max delta: 0.0117529683091 ==== learning rate was 0.0117529685795
iteration #1315: max delta: 0.0117438324529 ==== learning rate was 0.0117438325753
iteration #1316: max delta: 0.0117347088267 ==== learning rate was 0.0117347089312
iteration #1317: max delta: 4.74405443429e-10 ==== learning rate was 0.0117255976303
iteration #1318: max delta: 2.05727714352e-10 ==== learning rate was 0.0117164986559
iteration #1319: max delta: 0.011707411773 ==== learning rate was 0.0117074119915
iteration #1320: max delta: 0.0116983374189 ==== learning rate was 0.0116983376203
iteration #1321: max delta: 1.75539345363e-10 ==== learning rate was 0.0116892755256
iteration #1322: max delta: 1.80993764081e-10 ==== learning rate was 0.011680225691
iteration #1323: max delta: 7.36661961963e-10 ==== learning rate was 0.0116711880998
iteration #1324: max delta: 6.0154196243e-10 ==== learning rate was 0.0116621627354
iteration #1325: max delta: 0.011653149411 ==== learning rate was 0.0116531495813
iteration #1326: max delta: 0.0116441472472 ==== learning rate was 0.011644148621
iteration #1327: max delta: 5.84845847012e-10 ==== learning rate was 0.011635159838
iteration #1328: max delta: 0.0116261831276 ==== learning rate was 0.0116261832158
iteration #1329: max delta: 0.011617218548 ==== learning rate was 0.0116172187379
iteration #1330: max delta: 8.47102334147e-11 ==== learning rate was 0.011608266388
iteration #1331: max delta: 2.15006871025e-10 ==== learning rate was 0.0115993261496
iteration #1332: max delta: 0.0115903979217 ==== learning rate was 0.0115903980063
iteration #1333: max delta: 0.0115814818837 ==== learning rate was 0.0115814819418
iteration #1334: max delta: 1.95903960862e-10 ==== learning rate was 0.0115725779397
iteration #1335: max delta: 1.60762696961e-10 ==== learning rate was 0.0115636859838
iteration #1336: max delta: 4.02089694446e-10 ==== learning rate was 0.0115548060576
iteration #1337: max delta: 0.0115459379546 ==== learning rate was 0.011545938145
iteration #1338: max delta: 0.0115370819614 ==== learning rate was 0.0115370822297
iteration #1339: max delta: 2.88092409915e-10 ==== learning rate was 0.0115282382954
iteration #1340: max delta: 3.56494768766e-10 ==== learning rate was 0.011519406326
iteration #1341: max delta: 0.0115105861786 ==== learning rate was 0.0115105863052
iteration #1342: max delta: 0.0115017779299 ==== learning rate was 0.0115017782169
iteration #1343: max delta: 0.011492981898 ==== learning rate was 0.011492982045
iteration #1344: max delta: 0.0114841976425 ==== learning rate was 0.0114841977733
iteration #1345: max delta: 2.54743857647e-09 ==== learning rate was 0.0114754253857
iteration #1346: max delta: 2.7072474788e-10 ==== learning rate was 0.0114666648661
iteration #1347: max delta: 2.34374386224e-10 ==== learning rate was 0.0114579161986
iteration #1348: max delta: 0.0114491791073 ==== learning rate was 0.011449179367
iteration #1349: max delta: 7.36543921493e-10 ==== learning rate was 0.0114404543554
iteration #1350: max delta: 1.8521645929e-10 ==== learning rate was 0.0114317411477
iteration #1351: max delta: 0.0114230396079 ==== learning rate was 0.011423039728
iteration #1352: max delta: 0.0114143499169 ==== learning rate was 0.0114143500804
iteration #1353: max delta: 1.72684924501e-10 ==== learning rate was 0.0114056721889
iteration #1354: max delta: 0.0113970058618 ==== learning rate was 0.0113970060375
iteration #1355: max delta: 4.34456168492e-10 ==== learning rate was 0.0113883516105
iteration #1356: max delta: 0.0113797086038 ==== learning rate was 0.011379708892
iteration #1357: max delta: 1.64731799777e-10 ==== learning rate was 0.0113710778661
iteration #1358: max delta: 0.0113624581758 ==== learning rate was 0.0113624585169
iteration #1359: max delta: 0.0113538504682 ==== learning rate was 0.0113538508288
iteration #1360: max delta: 0.0113452546984 ==== learning rate was 0.011345254786
iteration #1361: max delta: 0.011336670013 ==== learning rate was 0.0113366703726
iteration #1362: max delta: 0.0113280973972 ==== learning rate was 0.0113280975729
iteration #1363: max delta: 5.81550172918e-11 ==== learning rate was 0.0113195363713
iteration #1364: max delta: 1.29965413126e-10 ==== learning rate was 0.011310986752
iteration #1365: max delta: 1.48932026038e-10 ==== learning rate was 0.0113024486993
iteration #1366: max delta: 0.0112939220669 ==== learning rate was 0.0112939221977
iteration #1367: max delta: 0.0112854070992 ==== learning rate was 0.0112854072315
iteration #1368: max delta: 0.0112769036886 ==== learning rate was 0.0112769037851
iteration #1369: max delta: 3.58640314894e-10 ==== learning rate was 0.0112684118429
iteration #1370: max delta: 0.0112599312181 ==== learning rate was 0.0112599313893
iteration #1371: max delta: 8.14975556505e-10 ==== learning rate was 0.0112514624089
iteration #1372: max delta: 0.0112430048243 ==== learning rate was 0.0112430048859
iteration #1373: max delta: 2.2361327412e-10 ==== learning rate was 0.0112345588051
iteration #1374: max delta: 0.0112261239689 ==== learning rate was 0.0112261241509
iteration #1375: max delta: 1.32525829809e-10 ==== learning rate was 0.0112177009078
iteration #1376: max delta: 0.011209289015 ==== learning rate was 0.0112092890604
iteration #1377: max delta: 0.0112008885404 ==== learning rate was 0.0112008885932
iteration #1378: max delta: 2.57965895498e-10 ==== learning rate was 0.011192499491
iteration #1379: max delta: 0.0111841216697 ==== learning rate was 0.0111841217383
iteration #1380: max delta: 8.66477801627e-11 ==== learning rate was 0.0111757553197
iteration #1381: max delta: 0.0111674000384 ==== learning rate was 0.0111674002199
iteration #1382: max delta: 0.0111590560729 ==== learning rate was 0.0111590564237
iteration #1383: max delta: 0.0111507238071 ==== learning rate was 0.0111507239156
iteration #1384: max delta: 1.1054284627e-10 ==== learning rate was 0.0111424026805
iteration #1385: max delta: 0.0111340921835 ==== learning rate was 0.0111340927031
iteration #1386: max delta: 1.77855160903e-10 ==== learning rate was 0.0111257939682
iteration #1387: max delta: 1.11513285803e-10 ==== learning rate was 0.0111175064605
iteration #1388: max delta: 0.011109229539 ==== learning rate was 0.0111092301649
iteration #1389: max delta: 0.0111009649854 ==== learning rate was 0.0111009650661
iteration #1390: max delta: 0.0110927110794 ==== learning rate was 0.0110927111491
iteration #1391: max delta: 0.0110844682965 ==== learning rate was 0.0110844683988
iteration #1392: max delta: 0.0110762366804 ==== learning rate was 0.0110762367999
iteration #1393: max delta: 0.0110680161333 ==== learning rate was 0.0110680163375
iteration #1394: max delta: 0.0110598069586 ==== learning rate was 0.0110598069964
iteration #1395: max delta: 1.07497235635e-10 ==== learning rate was 0.0110516087616
iteration #1396: max delta: 0.0110434215013 ==== learning rate was 0.0110434216181
iteration #1397: max delta: 0.0110352454428 ==== learning rate was 0.0110352455509
iteration #1398: max delta: 0.0110270804721 ==== learning rate was 0.011027080545
iteration #1399: max delta: 0.0110189262438 ==== learning rate was 0.0110189265855
iteration #1400: max delta: 0.0110107836018 ==== learning rate was 0.0110107836573
iteration #1401: max delta: 1.3026486322e-10 ==== learning rate was 0.0110026517455
iteration #1402: max delta: 1.64370854788e-10 ==== learning rate was 0.0109945308354
iteration #1403: max delta: 6.43810735706e-11 ==== learning rate was 0.0109864209119
iteration #1404: max delta: 1.19504328863e-10 ==== learning rate was 0.0109783219602
iteration #1405: max delta: 1.21339532321e-10 ==== learning rate was 0.0109702339655
iteration #1406: max delta: 1.15756426871e-10 ==== learning rate was 0.0109621569129
iteration #1407: max delta: 0.0109540906856 ==== learning rate was 0.0109540907877
iteration #1408: max delta: 1.01599492584e-10 ==== learning rate was 0.010946035575
iteration #1409: max delta: 0.0109379911372 ==== learning rate was 0.0109379912601
iteration #1410: max delta: 1.09605045799e-10 ==== learning rate was 0.0109299578283
iteration #1411: max delta: 4.52558751234e-10 ==== learning rate was 0.0109219352648
iteration #1412: max delta: 0.0109139233696 ==== learning rate was 0.0109139235549
iteration #1413: max delta: 0.010905922621 ==== learning rate was 0.010905922684
iteration #1414: max delta: 0.0108979325691 ==== learning rate was 0.0108979326373
iteration #1415: max delta: 0.0108899532718 ==== learning rate was 0.0108899534003
iteration #1416: max delta: 7.45709750049e-11 ==== learning rate was 0.0108819849583
iteration #1417: max delta: 4.25193936991e-11 ==== learning rate was 0.0108740272966
iteration #1418: max delta: 0.0108660803775 ==== learning rate was 0.0108660804008
iteration #1419: max delta: 1.3455665199e-10 ==== learning rate was 0.0108581442563
iteration #1420: max delta: 6.94477295775e-11 ==== learning rate was 0.0108502188484
iteration #1421: max delta: 0.0108423040269 ==== learning rate was 0.0108423041628
iteration #1422: max delta: 2.30889134769e-10 ==== learning rate was 0.0108344001848
iteration #1423: max delta: 4.9101025072e-11 ==== learning rate was 0.0108265069
iteration #1424: max delta: 7.91604166156e-11 ==== learning rate was 0.0108186242939
iteration #1425: max delta: 1.17711838536e-10 ==== learning rate was 0.0108107523521
iteration #1426: max delta: 0.0108028908067 ==== learning rate was 0.0108028910601
iteration #1427: max delta: 1.88519252934e-10 ==== learning rate was 0.0107950404036
iteration #1428: max delta: 0.0107872003025 ==== learning rate was 0.0107872003682
iteration #1429: max delta: 0.0107793708145 ==== learning rate was 0.0107793709394
iteration #1430: max delta: 0.0107715520592 ==== learning rate was 0.010771552103
iteration #1431: max delta: 0.010763743803 ==== learning rate was 0.0107637438445
iteration #1432: max delta: 0.0107559460088 ==== learning rate was 0.0107559461498
iteration #1433: max delta: 0.0107481589091 ==== learning rate was 0.0107481590044
iteration #1434: max delta: 1.58285847592e-10 ==== learning rate was 0.0107403823942
iteration #1435: max delta: 2.95755988462e-10 ==== learning rate was 0.0107326163049
iteration #1436: max delta: 8.83466258417e-11 ==== learning rate was 0.0107248607222
iteration #1437: max delta: 0.0107171155859 ==== learning rate was 0.0107171156319
iteration #1438: max delta: 9.41530441341e-11 ==== learning rate was 0.0107093810198
iteration #1439: max delta: 6.37357538025e-11 ==== learning rate was 0.0107016568719
iteration #1440: max delta: 6.02638136826e-11 ==== learning rate was 0.0106939431738
iteration #1441: max delta: 1.40403589455e-10 ==== learning rate was 0.0106862399115
iteration #1442: max delta: 4.85268919596e-11 ==== learning rate was 0.0106785470708
iteration #1443: max delta: 6.12898703339e-11 ==== learning rate was 0.0106708646377
iteration #1444: max delta: 4.9117183628e-11 ==== learning rate was 0.010663192598
iteration #1445: max delta: 7.39456954388e-11 ==== learning rate was 0.0106555309377
iteration #1446: max delta: 0.0106478795978 ==== learning rate was 0.0106478796428
iteration #1447: max delta: 5.17353606037e-11 ==== learning rate was 0.0106402386993
iteration #1448: max delta: 0.0106326080186 ==== learning rate was 0.010632608093
iteration #1449: max delta: 0.0106249876645 ==== learning rate was 0.0106249878102
iteration #1450: max delta: 4.71287423119e-10 ==== learning rate was 0.0106173778367
iteration #1451: max delta: 0.0106097776606 ==== learning rate was 0.0106097781586
iteration #1452: max delta: 0.0106021885986 ==== learning rate was 0.010602188762
iteration #1453: max delta: 3.14009415184e-10 ==== learning rate was 0.0105946096331
iteration #1454: max delta: 6.55816022438e-11 ==== learning rate was 0.0105870407578
iteration #1455: max delta: 0.010579482088 ==== learning rate was 0.0105794821224
iteration #1456: max delta: 8.43385316988e-11 ==== learning rate was 0.010571933713
iteration #1457: max delta: 0.0105643954 ==== learning rate was 0.0105643955157
iteration #1458: max delta: 0.0105568673778 ==== learning rate was 0.0105568675167
iteration #1459: max delta: 0.0105493496654 ==== learning rate was 0.0105493497023
iteration #1460: max delta: 7.10811504012e-11 ==== learning rate was 0.0105418420586
iteration #1461: max delta: 0.0105343445354 ==== learning rate was 0.0105343445719
iteration #1462: max delta: 5.21810183628e-10 ==== learning rate was 0.0105268572285
iteration #1463: max delta: 0.0105193798425 ==== learning rate was 0.0105193800146
iteration #1464: max delta: 7.79181161904e-11 ==== learning rate was 0.0105119129165
iteration #1465: max delta: 7.21563664687e-11 ==== learning rate was 0.0105044559206
iteration #1466: max delta: 8.95369079589e-11 ==== learning rate was 0.0104970090132
iteration #1467: max delta: 0.0104895721485 ==== learning rate was 0.0104895721805
iteration #1468: max delta: 6.68793393812e-11 ==== learning rate was 0.0104821454091
iteration #1469: max delta: 0.0104747285869 ==== learning rate was 0.0104747286852
iteration #1470: max delta: 5.49949411196e-11 ==== learning rate was 0.0104673219954
iteration #1471: max delta: 4.73255484272e-11 ==== learning rate was 0.0104599253259
iteration #1472: max delta: 0.0104525385262 ==== learning rate was 0.0104525386633
iteration #1473: max delta: 7.9945403959e-11 ==== learning rate was 0.0104451619941
iteration #1474: max delta: 0.0104377952587 ==== learning rate was 0.0104377953046
iteration #1475: max delta: 0.0104304385176 ==== learning rate was 0.0104304385814
iteration #1476: max delta: 0.0104230917593 ==== learning rate was 0.0104230918111
iteration #1477: max delta: 1.49356401936e-10 ==== learning rate was 0.0104157549801
iteration #1478: max delta: 2.1503252099e-10 ==== learning rate was 0.010408428075
iteration #1479: max delta: 0.0104011110342 ==== learning rate was 0.0104011110823
iteration #1480: max delta: 9.96416226741e-11 ==== learning rate was 0.0103938039888
iteration #1481: max delta: 4.30799891608e-11 ==== learning rate was 0.0103865067809
iteration #1482: max delta: 0.0103792194197 ==== learning rate was 0.0103792194453
iteration #1483: max delta: 3.51424691592e-10 ==== learning rate was 0.0103719419687
iteration #1484: max delta: 0.0103646743235 ==== learning rate was 0.0103646743376
iteration #1485: max delta: 3.35323702281e-11 ==== learning rate was 0.0103574165389
iteration #1486: max delta: 0.0103501685164 ==== learning rate was 0.0103501685591
iteration #1487: max delta: 0.010342930321 ==== learning rate was 0.010342930385
iteration #1488: max delta: 7.01378700241e-11 ==== learning rate was 0.0103357020034
iteration #1489: max delta: 0.0103284833812 ==== learning rate was 0.0103284834009
iteration #1490: max delta: 2.30770390352e-10 ==== learning rate was 0.0103212745644
iteration #1491: max delta: 5.75108241457e-11 ==== learning rate was 0.0103140754807
iteration #1492: max delta: 3.6385818502e-11 ==== learning rate was 0.0103068861364
iteration #1493: max delta: 0.0102997064809 ==== learning rate was 0.0102997065186
iteration #1494: max delta: 0.0102925365813 ==== learning rate was 0.010292536614
iteration #1495: max delta: 1.18629826358e-10 ==== learning rate was 0.0102853764094
iteration #1496: max delta: 1.14956756097e-10 ==== learning rate was 0.0102782258918
iteration #1497: max delta: 1.2980733591e-10 ==== learning rate was 0.010271085048
iteration #1498: max delta: 4.94601689066e-11 ==== learning rate was 0.010263953865
iteration #1499: max delta: 3.07064851155e-11 ==== learning rate was 0.0102568323297
iteration #1500: max delta: 1.30491970197e-10 ==== learning rate was 0.010249720429
iteration #1501: max delta: 0.0102426181308 ==== learning rate was 0.0102426181499
iteration #1502: max delta: 0.0102355253992 ==== learning rate was 0.0102355254794
iteration #1503: max delta: 4.57286926029e-11 ==== learning rate was 0.0102284424045
iteration #1504: max delta: 0.0102213688749 ==== learning rate was 0.0102213689121
iteration #1505: max delta: 3.38687563452e-11 ==== learning rate was 0.0102143049894
iteration #1506: max delta: 0.0102072505221 ==== learning rate was 0.0102072506234
iteration #1507: max delta: 2.04481119883e-10 ==== learning rate was 0.0102002058012
iteration #1508: max delta: 0.0101931704841 ==== learning rate was 0.0101931705098
iteration #1509: max delta: 4.51192447123e-11 ==== learning rate was 0.0101861447364
iteration #1510: max delta: 2.40704317125e-11 ==== learning rate was 0.010179128468
iteration #1511: max delta: 0.0101721216671 ==== learning rate was 0.0101721216919
iteration #1512: max delta: 4.94507320874e-11 ==== learning rate was 0.0101651243951
iteration #1513: max delta: 6.2594098744e-11 ==== learning rate was 0.0101581365649
iteration #1514: max delta: 0.0101511580129 ==== learning rate was 0.0101511581884
iteration #1515: max delta: 0.0101441891334 ==== learning rate was 0.0101441892529
iteration #1516: max delta: 1.64096651943e-11 ==== learning rate was 0.0101372297456
iteration #1517: max delta: 1.07708512379e-10 ==== learning rate was 0.0101302796537
iteration #1518: max delta: 0.0101233389367 ==== learning rate was 0.0101233389645
iteration #1519: max delta: 0.0101164076181 ==== learning rate was 0.0101164076652
iteration #1520: max delta: 6.25531361087e-11 ==== learning rate was 0.0101094857433
iteration #1521: max delta: 0.0101025731468 ==== learning rate was 0.0101025731859
iteration #1522: max delta: 6.81089620816e-11 ==== learning rate was 0.0100956699804
iteration #1523: max delta: 0.0100887760704 ==== learning rate was 0.0100887761142
iteration #1524: max delta: 0.0100818914004 ==== learning rate was 0.0100818915746
iteration #1525: max delta: 0.010075016315 ==== learning rate was 0.010075016349
iteration #1526: max delta: 0.0100681503405 ==== learning rate was 0.0100681504248
iteration #1527: max delta: 0.0100612937488 ==== learning rate was 0.0100612937894
iteration #1528: max delta: 3.98982780629e-11 ==== learning rate was 0.0100544464303
iteration #1529: max delta: 0.0100476082158 ==== learning rate was 0.0100476083348
iteration #1530: max delta: 0.0100407794593 ==== learning rate was 0.0100407794906
iteration #1531: max delta: 0.0100339598387 ==== learning rate was 0.010033959885
iteration #1532: max delta: 3.12207745345e-11 ==== learning rate was 0.0100271495056
iteration #1533: max delta: 0.0100203483019 ==== learning rate was 0.0100203483398
iteration #1534: max delta: 0.010013556324 ==== learning rate was 0.0100135563753
iteration #1535: max delta: 0.0100067735347 ==== learning rate was 0.0100067735995
iteration #1536: max delta: 0.00999999998284 ==== learning rate was 0.01
iteration #1537: max delta: 8.07047950751e-11 ==== learning rate was 0.00999323556445
iteration #1538: max delta: 0.00998648025716 ==== learning rate was 0.00998648028043
iteration #1539: max delta: 1.19365073864e-10 ==== learning rate was 0.00997973413554
iteration #1540: max delta: 2.99871082918e-11 ==== learning rate was 0.00997299711742
iteration #1541: max delta: 3.47217955682e-11 ==== learning rate was 0.00996626921372
iteration #1542: max delta: 0.00995955038257 ==== learning rate was 0.00995955041213
iteration #1543: max delta: 3.26877471842e-11 ==== learning rate was 0.00995284070031
iteration #1544: max delta: 0.00994613997733 ==== learning rate was 0.00994614006597
iteration #1545: max delta: 0.00993944845174 ==== learning rate was 0.00993944849684
iteration #1546: max delta: 0.00993276577578 ==== learning rate was 0.00993276598064
iteration #1547: max delta: 0.00992609246426 ==== learning rate was 0.00992609250513
iteration #1548: max delta: 0.00991942803836 ==== learning rate was 0.00991942805808
iteration #1549: max delta: 0.00991277258475 ==== learning rate was 0.00991277262728
iteration #1550: max delta: 0.00990612616888 ==== learning rate was 0.00990612620052
iteration #1551: max delta: 1.22823460542e-11 ==== learning rate was 0.00989948876563
iteration #1552: max delta: 0.00989286029604 ==== learning rate was 0.00989286031044
iteration #1553: max delta: 0.00988624079925 ==== learning rate was 0.0098862408228
iteration #1554: max delta: 4.60365712276e-11 ==== learning rate was 0.00987963029058
iteration #1555: max delta: 0.0098730286823 ==== learning rate was 0.00987302870166
iteration #1556: max delta: 6.95160473578e-11 ==== learning rate was 0.00986643604395
iteration #1557: max delta: 0.00985985229292 ==== learning rate was 0.00985985230536
iteration #1558: max delta: 0.00985327745096 ==== learning rate was 0.00985327747382
iteration #1559: max delta: 0.00984671146771 ==== learning rate was 0.00984671153729
iteration #1560: max delta: 0.00984015442542 ==== learning rate was 0.00984015448373
iteration #1561: max delta: 0.00983360626866 ==== learning rate was 0.00983360630112
iteration #1562: max delta: 2.22942154322e-11 ==== learning rate was 0.00982706697747
iteration #1563: max delta: 0.00982053641011 ==== learning rate was 0.00982053650078
iteration #1564: max delta: 0.00981401480409 ==== learning rate was 0.00981401485909
iteration #1565: max delta: 0.00980750200452 ==== learning rate was 0.00980750204044
iteration #1566: max delta: 1.86307862518e-11 ==== learning rate was 0.00980099803291
iteration #1567: max delta: 0.00979450281453 ==== learning rate was 0.00979450282456
iteration #1568: max delta: 1.99146567999e-11 ==== learning rate was 0.00978801640349
iteration #1569: max delta: 0.00978153872237 ==== learning rate was 0.00978153875782
iteration #1570: max delta: 0.00977506984799 ==== learning rate was 0.00977506987568
iteration #1571: max delta: 0.00976860972823 ==== learning rate was 0.0097686097452
iteration #1572: max delta: 2.39982709862e-11 ==== learning rate was 0.00976215835454
iteration #1573: max delta: 1.8872234773e-11 ==== learning rate was 0.0097557156919
iteration #1574: max delta: 0.00974928172724 ==== learning rate was 0.00974928174544
iteration #1575: max delta: 0.00974285647862 ==== learning rate was 0.00974285650339
iteration #1576: max delta: 4.53154492304e-11 ==== learning rate was 0.00973643995397
iteration #1577: max delta: 4.15772201267e-11 ==== learning rate was 0.00973003208541
iteration #1578: max delta: 1.31225907558e-11 ==== learning rate was 0.00972363288598
iteration #1579: max delta: 0.0097172421653 ==== learning rate was 0.00971724234394
iteration #1580: max delta: 0.00971086041185 ==== learning rate was 0.00971086044758
iteration #1581: max delta: 0.0097044871172 ==== learning rate was 0.00970448718521
iteration #1582: max delta: 0.00969812248613 ==== learning rate was 0.00969812254514
iteration #1583: max delta: 0.00969176648558 ==== learning rate was 0.00969176651571
iteration #1584: max delta: 1.99205404683e-11 ==== learning rate was 0.00968541908528
iteration #1585: max delta: 1.09289221791e-11 ==== learning rate was 0.00967908024219
iteration #1586: max delta: 0.0096727499684 ==== learning rate was 0.00967274997485
iteration #1587: max delta: 0.00966642808878 ==== learning rate was 0.00966642827165
iteration #1588: max delta: 1.25610616677e-11 ==== learning rate was 0.00966011512099
iteration #1589: max delta: 1.14507218186e-11 ==== learning rate was 0.00965381051132
iteration #1590: max delta: 0.00964751440975 ==== learning rate was 0.00964751443107
iteration #1591: max delta: 1.24094356912e-11 ==== learning rate was 0.00964122686871
iteration #1592: max delta: 0.00963494774319 ==== learning rate was 0.00963494781271
iteration #1593: max delta: 1.35609308104e-10 ==== learning rate was 0.00962867725156
iteration #1594: max delta: 1.48361551618e-10 ==== learning rate was 0.00962241517378
iteration #1595: max delta: 0.00961616154057 ==== learning rate was 0.00961616156787
iteration #1596: max delta: 1.08433677357e-11 ==== learning rate was 0.0096099164224
iteration #1597: max delta: 4.96465810335e-11 ==== learning rate was 0.00960367972589
iteration #1598: max delta: 0.00959745144148 ==== learning rate was 0.00959745146694
iteration #1599: max delta: 0.00959123162068 ==== learning rate was 0.00959123163412
iteration #1600: max delta: 0.00958502020022 ==== learning rate was 0.00958502021602
iteration #1601: max delta: 0.0095788171772 ==== learning rate was 0.00957881720128
iteration #1602: max delta: 4.17809708857e-11 ==== learning rate was 0.00957262257851
iteration #1603: max delta: 1.01092284588e-10 ==== learning rate was 0.00956643633637
iteration #1604: max delta: 0.00956025845638 ==== learning rate was 0.00956025846352
iteration #1605: max delta: 9.76108026632e-12 ==== learning rate was 0.00955408894863
iteration #1606: max delta: 2.47616096763e-11 ==== learning rate was 0.0095479277804
iteration #1607: max delta: 3.2083911678e-11 ==== learning rate was 0.00954177494753
iteration #1608: max delta: 0.00953563042169 ==== learning rate was 0.00953563043875
iteration #1609: max delta: 0.00952949416956 ==== learning rate was 0.0095294942428
iteration #1610: max delta: 0.00952336632991 ==== learning rate was 0.00952336634843
iteration #1611: max delta: 3.11112347214e-11 ==== learning rate was 0.00951724674441
iteration #1612: max delta: 2.46143007817e-11 ==== learning rate was 0.00951113541952
iteration #1613: max delta: 6.63073722883e-11 ==== learning rate was 0.00950503236256
iteration #1614: max delta: 2.48213402133e-11 ==== learning rate was 0.00949893756235
iteration #1615: max delta: 0.00949285100065 ==== learning rate was 0.00949285100772
iteration #1616: max delta: 0.00948677267666 ==== learning rate was 0.00948677268751
iteration #1617: max delta: 0.00948070255443 ==== learning rate was 0.00948070259058
iteration #1618: max delta: 2.03748497238e-11 ==== learning rate was 0.0094746407058
iteration #1619: max delta: 0.0094685869209 ==== learning rate was 0.00946858702208
iteration #1620: max delta: 1.18221068031e-11 ==== learning rate was 0.0094625415283
iteration #1621: max delta: 1.6429554351e-11 ==== learning rate was 0.00945650421339
iteration #1622: max delta: 4.28481187354e-11 ==== learning rate was 0.00945047506629
iteration #1623: max delta: 4.77691056539e-11 ==== learning rate was 0.00944445407594
iteration #1624: max delta: 4.87199492876e-11 ==== learning rate was 0.00943844123132
iteration #1625: max delta: 1.62948774446e-11 ==== learning rate was 0.00943243652139
iteration #1626: max delta: 3.12404149258e-11 ==== learning rate was 0.00942643993516
iteration #1627: max delta: 3.59994909456e-11 ==== learning rate was 0.00942045146163
iteration #1628: max delta: 4.06842613252e-11 ==== learning rate was 0.00941447108983
iteration #1629: max delta: 7.66211941593e-11 ==== learning rate was 0.0094084988088
================ recent 10 deltas ================
[1.1822106803050405e-11, 1.642955435095226e-11, 4.284811873538376e-11, 4.776910565393038e-11, 4.871994928761673e-11, 1.6294877444644457e-11, 3.124041492582575e-11, 3.599949094557515e-11, 4.068426132521334e-11, 7.662119415931481e-11]
============================= weights =====================================
[14.862120852093954, -0.3068340424467362, -0.5645102324261682, -0.5919559154002827, -0.6359521475922265, -0.8748815468892239, -0.4930407319865049, -0.5192237281350535, -0.6559887144139089, -0.6135184239245799, -0.5000423028157083, -0.5893203095213102, -0.47421824502506327, -0.5062396524322125, -0.7225858119307782, -0.8418707566299786, -0.4067223814493749, -0.3887836292866165, -1.1022047140919125, -0.7037658946550015, -0.38782066586006514, -0.2673175014104722, -0.6081460744947886, -0.7260299514299328, -0.7701833988239132, -0.7844040343754791, -0.5674813421965329, -0.6592163318764798, -0.782373449386169, -0.6756008830884719, -0.38810283789551325, -0.49573283156700765, -0.5565480819552907, -0.4233849129404532, -0.6866249586606149, -0.942730478345319, -0.44457529304206433, -0.3395950090799228, -1.0289972891813013, -0.6775480823792572, -0.2345973516195558, -0.2971783680596982, -0.8202607171059093, -0.8419641149305267, -0.9571630252278982, -0.7106034838502338, -0.7275304927400609, -0.8308606933011238, -0.8313687918335759, -0.7880890997852897, -0.24865811386406916, -0.32046043592714624, -0.7750730247996082, -0.37792949860018404, -0.5268967874788394, -0.9838331550393444, -0.5382814518071383, -0.40888660292559964, -1.0434656080421754, -0.6383827883754704, -0.14296756621174211, -0.29955405587812983, -0.635774700304924, -0.658293127991988, -0.7056795720337099, -0.8623305508327862, -0.5360030479949217, -0.5702834623984521, -0.8112868939519353, -0.6982666127062951, -0.45951235989296424, -0.5644537034243267, -0.5729699179522403, -0.4934420229057796, -0.6840522383234774, -0.9300190289102966, -0.43464555588488873, -0.36374657173909397, -1.058444680522129, -0.658876917953763, -0.3209963641638313, -0.2725481201379408, -0.5370703675909478, -0.5864056080441187, -0.6381152991298793, -0.8911546351416636, -0.43835700520997056, -0.48039769501477314, -0.6522588864040227, -0.579370385899744, -0.6270229894223799, -0.6553513221862953, -0.43171975400025875, -0.5527034201451749, -0.7612109075704986, -0.8715088557530678, -0.37045135973126847, -0.3764505707895551, -1.0266825870434748, -0.6477946573351032, -0.5401732888750778, -0.22935157981328855, -0.46163552422059634, -0.52494073112021, -0.5186123352469275, -0.851482757921263, -0.4034664522620266, -0.37485811248634715, -0.5689470643496338, -0.5524490695347273, -0.6026261077040717, -0.632293033114486, -0.36927585116671724, -0.5206724295636918, -0.819106086846733, -0.8601078114164373, -0.38846414899706544, -0.37008558029451216, -1.1052898489781595, -0.6782881331165902, -0.4230513615614961, -0.2655168652825049, -0.4549173250861444, -0.5328051732449073, -0.5595455418112547, -0.8933479746895162, -0.41824476310192416, -0.4393369603708605, -0.7394503204376078, -0.5726206122446678, -0.4878254376395328, -0.5412159530039465, -0.4012026308896511, -0.5091434567054046, -0.7006775692842923, -0.8051664131632604, -0.3900219575859059, -0.3275434512708515, -0.998391559878311, -0.5997998555174555, -0.3539545353238802, -0.2907433797572786, -0.7202704133415879, -0.8016494779228029, -0.9045629211139206, -0.7524811502714239, -0.6240268044696639, -0.7398345845164525, -0.8517072885428768, -0.7428198322940265, -0.2470007821114372, -0.34659313103375006, -0.6768083630881659, -0.37164790146589183, -0.5746864563552234, -0.8757203789313087, -0.5531760684414949, -0.42990009397329665, -1.074797290649172, -0.6326278010709231, -0.17466282511730544, -0.3222467172900929, -0.5455829994134067, -0.6244041197361697, -0.6801017685153092, -0.8533423867813058, -0.4887570633403971, -0.5413205621046804, -0.8045174102918962, -0.5821368955369175, -0.4170819589112943, -0.4776618509231465, -0.48170448958841927, -0.4812423178598619, -0.649461295994092, -0.8479289118410348, -0.46195745159873736, -0.3412058266012254, -1.0258541407272794, -0.6127633521247882, -0.2765058429895548, -0.24313758932377916, -0.48924094211931457, -0.4989886611584737, -0.49196347432096, -0.9664212764498331, -0.39230155155475627, -0.4041978919394768, -0.6187359622471781, -0.601739510463288, -0.5972147387224545, -0.6516094742849202, -0.3838024658387346, -0.5649166860074479, -0.8627911391225757, -0.6848822856379218, -0.3666018421248239, -0.3996698383437451, -1.1263752770859805, -0.726085039110758, -0.48267986513611094]
